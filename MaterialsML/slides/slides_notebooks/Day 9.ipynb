{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f8e3e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Materials+ML Workshop Day 9\n",
    "\n",
    "![logo](logo.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea039bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Content for today:\n",
    "\n",
    "* Unsupervised Learning review:\n",
    "    * Correlation Matrices\n",
    "    * Dimensionality reduction\n",
    "    * Principal Components Analysis (PCA)\n",
    "    * Clustering\n",
    "    * Distribution Estimation\n",
    "    \n",
    "* Neural networks:\n",
    "    * Introduction to Neural Networks\n",
    "    * Neuron Models\n",
    "    * Activation Functions\n",
    "    * Training Neural Networks\n",
    "    \n",
    "* Application:\n",
    "    * Training a basic neural network with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ccbbf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Background Survey\n",
    "\n",
    "![background survey](survey_qr.png)\n",
    "\n",
    "## [https://forms.gle/FEWqPavJJYC9VzfH7](https://forms.gle/FEWqPavJJYC9VzfH7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b71a1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## The Workshop Online Book:\n",
    "\n",
    "### [https://cburdine.github.io/materials-ml-workshop/](https://cburdine.github.io/materials-ml-workshop/)\n",
    "\n",
    "* **Recordings of Previous Workshop Sessions (and slides) now available!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3466e435",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tentative Workshop Schedule:\n",
    "\n",
    "| Session       | Date          | Content                                                       |\n",
    "| -------       | ----          | -------                                                       |\n",
    "| Day 0         | 06/16/2023 (2:30-3:30 PM) | Introduction, Setting up your Python Notebook     |\n",
    "| Day 1         | 06/19/2023 (2:30-3:30 PM) | Python Data Types                                 |\n",
    "| Day 2         | 06/20/2023 (2:30-3:30 PM) | Python Functions and Classes                      |\n",
    "| Day 3     | 06/21/2023 (2:30-3:30 PM) | Scientific Computing with Numpy and Scipy         |\n",
    "| Day 4         | 06/22/2023 (2:30-3:30 PM) | Data Manipulation and Visualization              |\n",
    "| Day 5       | 06/23/2023 (2:30-3:30 PM) | Materials Science Packages                     |\n",
    "| Day 6       | 06/26/2023 (2:30-3:30 PM) | Introduction to ML, Supervised Learning           |\n",
    "| Day 7         | 06/27/2023 (2:30-3:30 PM) | Regression Models                               |\n",
    "| Day 8         | 06/28/2023 (2:30-3:30 PM) | Unsupervised Learning                             |\n",
    "| **Day 9**         | **06/29/2023 (2:30-3:30 PM)** | **Neural Networks**                                   |\n",
    "| Day 10        | 06/30/2023 (2:30-3:30 PM) | Advanced Applications in Materials Science        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572813e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions\n",
    "\n",
    "* Unsupervised Learning review:\n",
    "    * Feature selection\n",
    "    * Correlation Matrices\n",
    "    * Dimensionality reduction\n",
    "    * Principal Components Analysis (PCA)\n",
    "    * Clustering\n",
    "    * Distribution Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82158c48",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Learning Models:\n",
    "\n",
    "* Models applied to unlabeled data with the goal of discovering trends, patterns, extracting features, or finding relationships between data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e74e48",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![unsupervised learning](unsupervised_learning.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7072d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Importance of Dimensionality\n",
    "\n",
    "* Dimensionality is an important concept in materials science.\n",
    "    * The dimensionality of a material affects its properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe93842",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Sometimes, data can be confined to some low-dimensional manifold embedded in a higher-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669549cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example: The \"Swiss Roll\" manifold**\n",
    "\n",
    "![Swiss roll](swiss_roll.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf1e1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Correlation Matrix:\n",
    "\n",
    "* Recall that it is generally a good idea to normalize our data:\n",
    "\n",
    "$$\\mathbf{x} \\mapsto \\mathbf{z}:\\quad z_i = \\frac{x_i - \\mu_i}{\\sigma_i}$$\n",
    "\n",
    "* The _correlation matrix_ (denoted $\\bar{\\Sigma}$) is the covariance matrix of the normalized data:\n",
    "\n",
    "$$ \\bar{\\Sigma} = \\frac{1}{N} \\sum_{n=1}^N \\mathbf{z}_n\\mathbf{z}_n^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28159c9b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Components Analysis (PCA)\n",
    "\n",
    "* The eigenvectors of the correlation matrix are called _principal components_.\n",
    "\n",
    "* The associated eigenvalues describe the proportion of the data variance in the direction of each principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62623b77",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\bar{\\Sigma} = P D P^{T}$$\n",
    "\n",
    "* $D$: Diagonal matrix (eigenvalues along diagonal)\n",
    "* $P$: Principal component matrix (columns are principal components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454f6aba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dimension reduction with PCA\n",
    "\n",
    "We can project our (normalized) data onto the first $n$ principal components to reduce the dimensionality of the data, while still keeping most of the variance:\n",
    "\n",
    "$$\\mathbf{z} \\mapsto \\mathbf{u} = \\begin{bmatrix}\n",
    "\\mathbf{z}^T\\mathbf{p}^{(1)} \\\\\n",
    "\\mathbf{z}^T\\mathbf{p}^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{z}^T\\mathbf{p}^{(n)} \\\\\n",
    "\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e37ebf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Means Clustering:\n",
    "\n",
    "* Identifies the centerpoints for a specified number of clusters $k$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7c9d2b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![kmeans](kmeans.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59905723",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernel Density Estimation:\n",
    "\n",
    "\n",
    "* Estimates the distribution of data as a sum of multivariate normal \"bumps\" at the position of each datapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c2219e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![kde](kernel_density_estimation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0890e176",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today's Content:\n",
    "\n",
    "**Neural Networks**\n",
    "\n",
    "* Introduction to Neural Networks\n",
    "    * Neuron Models\n",
    "    * Activation Functions\n",
    "    * Training Neural Networks\n",
    "    \n",
    "* Application:\n",
    "    * Training a basic neural network with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b94d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks\n",
    "\n",
    "* Neural networks are supervised machine learning models inspired by the functionality of networks of multipolar neurons in the brain:\n",
    "\n",
    "![multipolar_neuron](./multipolar_neuron.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec217bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What can neural networks do?\n",
    "\n",
    "* They are flexible non-linear models capable of solving many difficult supervised learning problems\n",
    "\n",
    "* They often work best on large, complex datasets\n",
    "\n",
    "* This predictive power comes at the cost of model interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe376bd2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * We know how the model computes predictions, but coming up with a general answer as to _why_ a neural network makes a particular prediction is very hard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe76efc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: The AlphaGo Model\n",
    "\n",
    "![alpha go](alphago.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6939d429",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standard Feed-Forward Neural Network\n",
    "\n",
    "* Neural Networks typically consist of individual collections of \"neurons\" that are stacked into sequential layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e52032",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Example: Standard \"feed-forward\" neural network\"\n",
    "\n",
    "![feedforward neural network](./simple_nn_overview.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c498ef18",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Single Neuron:\n",
    "\n",
    "* We have alreacy encountered a simple model of a neuron in the form of the Perceptron classifier model:\n",
    "\n",
    "$$f(\\mathbf{x}) = \\text{sign}\\left( w_0 + \\sum_{i=1}^D w_ix_i \\right)$$\n",
    "\n",
    "($\\text{sign}(x) = \\pm 1$, depending on the sign of $x$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ceef2e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $f(x) = +1$ only if a weighted sum of the inputs $x_i$ exceed a given threshold (i.e. $-w_0$)\n",
    "\n",
    "* This is similar to the electrical response of a neuron to external stimuli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9f20bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The Perceptron neuron model has some disadvantages:\n",
    "\n",
    "    * the function $\\text{sign}(x)$ is not continuous and has a derivative of 0 everywhere.\n",
    "   \n",
    "    * It can be difficult to fit this function to data if it is not continuous and differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff8cbe0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Neuron Activation Function\n",
    "\n",
    "Instead of the $\\text{sign}(x)$ function, we apply a continuous, non-linear function $\\sigma(x)$ to the output:\n",
    "\n",
    "![neuron](neuron.svg)\n",
    "\n",
    "* The function $\\sigma(x)$ is called the neuron's _activation function_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9835058d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The general form of a single neuron can be written as follows:\n",
    "\n",
    "$$f(\\mathbf{x}) = \\sigma(\\mathbf{w}^T\\underline{\\mathbf{x}}) = \\sigma\\left( w_0 + \\sum_{i=1}^D w_ix_i \\right)$$\n",
    "\n",
    "* Recall: $\\underline{\\mathbf{x}} = \\begin{bmatrix} 1 & x_1 & x_2 & \\dots & x_D \\end{bmatrix}^T$\n",
    "* Also: $\\mathbf{w} = \\begin{bmatrix} w_0 & w_1 & w_2 & \\dots & x_D \\end{bmatrix}^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9d300b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We can choose different activations $\\sigma(x)$, depending on the desired output range of the neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea416d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Common Activation Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf7d2ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* _Sigmoid function_: \n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "* _Hyperbolic Tangent_: \n",
    "\n",
    "$$\\sigma(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "* _Rectified Linear Unit (ReLU)_: \n",
    "\n",
    "$$\\sigma(x) = \\begin{cases} x & x > 0 \\\\ 0 & x \\le 0 \\end{cases}$$\n",
    "\n",
    "* _Leaky ReLU_: \n",
    "\n",
    "$$\\sigma(x) = \\begin{cases} x & x > 0 \\\\ \\alpha x & x \\le 0\\end{cases}\\qquad (0 < \\alpha \\ll 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f00fb5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing Activation Functions\n",
    "\n",
    "![activations](activations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b37d4d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Layer of Neurons:\n",
    "\n",
    "* We can combine multiple independent neurons into a layer of neurons.\n",
    "\n",
    "* The layer computes a vector $\\mathbf{a} = f(\\mathbf{x})$ of outputs from the neurons:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800a9b25",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\mathbf{a} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_m \\end{bmatrix} = f(\\mathbf{x}) = \\begin{bmatrix} \n",
    "\\sigma\\left(w_{1,0} + \\sum_{i=1}^D w_{1,i}x_i\\right) \\\\ \n",
    "\\sigma\\left(w_{2,0} + \\sum_{i=1}^D w_{2,i}x_i\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\sigma\\left(w_{m,0} + \\sum_{i=1}^D w_{m,i}x_i\\right)\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c138bcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Consider a layer of $m$ neurons each with $D+1$ weights.\n",
    "\n",
    "* We can organize the layer's weights into a matrix $\\mathbf{W}$:\n",
    "\n",
    "$$\\mathbf{W} = \\begin{bmatrix}\n",
    "w_{10} & w_{11} & \\dots & w_{1D} \\\\\n",
    "w_{20} & w_{12} & \\dots & w_{2D} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{m0} & w_{m1} & \\dots  & w_{mD}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b560ebe1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In terms of the weight matrix, we can write the neuron layer function as:\n",
    "\n",
    "$$\\mathbf{a} = f(\\mathbf{x}) = \\sigma(\\mathbf{W}\\mathbf{x})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b192f223",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Standard Feed-Forward Neural Network\n",
    "\n",
    "![FeedForward Neural Network](simple_nn.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e07eb8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Neural Networks\n",
    "\n",
    "* We train neural networks through _gradient descent_\n",
    "\n",
    "$$\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\eta \\frac{-\\nabla_w \\mathcal{E}(f)}{\\lVert{-\\nabla_w \\mathcal{E}(f)}\\rVert}$$\n",
    "\n",
    "* $\\eta$ is a constant called the _learning rate_.\n",
    "\n",
    "* The numerical process by which $\\nabla_w \\mathcal{E}(f)$ for layered neural networks is computed is called _backpropagation_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be3e589",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Tutorial: Basic Neural Network in Pytorch\n",
    "\n",
    "![pytorch](Pytorch_logo.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5299cf",
   "metadata": {},
   "source": [
    "### Goal: Train a neural network to learn the function:\n",
    "\n",
    "$$f(x_1, x_2) = \\frac{\\sin(\\sqrt{x_1^2 + x_2^2})}{\\sqrt{x_1^2 + x_2^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4261f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recommended Reading:\n",
    "\n",
    "(None)\n",
    "\n",
    "Note: some sections of the online book are still in progress ☹️"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
