{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f8e3e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Materials+ML Workshop Day 6\n",
    "\n",
    "![logo](logo.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea039bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Content for today:\n",
    "\n",
    "* Types of Machine Learning\n",
    "    * Supervised/Unsupervised/Reinforcement Learning\n",
    "* Supervised Learning\n",
    "    * Model Validity\n",
    "    * Regression, Logistic Regression, Classification\n",
    "    * Train/validation/Test sets\n",
    "* Training Supervised Learning Models\n",
    "    * Loss functions\n",
    "    * Gradient Descent (if time)\n",
    "    * Classification Problems (if time)\n",
    "* Application: Classifying Perovskites\n",
    "    * Installing scikit-learn\n",
    "    * scikit-learn models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ccbbf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Background Survey\n",
    "\n",
    "![background survey](survey_qr.png)\n",
    "\n",
    "## [https://forms.gle/FEWqPavJJYC9VzfH7](https://forms.gle/FEWqPavJJYC9VzfH7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b71a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Workshop Online Book:\n",
    "\n",
    "### [https://cburdine.github.io/materials-ml-workshop/](https://cburdine.github.io/materials-ml-workshop/)\n",
    "\n",
    "* **Recordings of Previous Workshop Sessions (and slides) now available!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3466e435",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tentative Workshop Schedule:\n",
    "\n",
    "| Session       | Date          | Content                                                       |\n",
    "| -------       | ----          | -------                                                       |\n",
    "| Day 0         | 06/16/2023 (2:30-3:30 PM) | Introduction, Setting up your Python Notebook     |\n",
    "| Day 1         | 06/19/2023 (2:30-3:30 PM) | Python Data Types                                 |\n",
    "| Day 2         | 06/20/2023 (2:30-3:30 PM) | Python Functions and Classes                      |\n",
    "| Day 3     | 06/21/2023 (2:30-3:30 PM) | Scientific Computing with Numpy and Scipy         |\n",
    "| Day 4         | 06/22/2023 (2:30-3:30 PM) | Data Manipulation and Visualization              |\n",
    "| Day 5       | 06/23/2023 (2:30-3:30 PM) | Materials Science Packages                     |\n",
    "| **Day 6**       | **06/26/2023 (2:30-3:30 PM)** | **Introduction to ML, Supervised Learning**           |\n",
    "| Day 7         | 06/27/2023 (2:30-3:30 PM) | Regression Models                                 |\n",
    "| Day 8         | 06/28/2023 (2:30-3:30 PM) | Unsupervised Learning                             |\n",
    "| Day 9         | 06/29/2023 (2:30-3:30 PM) | Neural Networks                                   |\n",
    "| Day 10        | 06/30/2023 (2:30-3:30 PM) | Advanced Applications in Materials Science        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572813e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions about review material:\n",
    "\n",
    "* Intro to ML Content:\n",
    "    * Statistics Review\n",
    "    * Linear Algebra Review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0408f890",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning\n",
    "\n",
    "**What is Machine Learning?**\n",
    "\n",
    "Machine Learning (ML) is a subfield of AI (Artificial Intelligence), that is concerned with:\n",
    "\n",
    "* Developing computational models that make predictions, identify trends, etc.\n",
    "* Methods that can be applied to improve these models based on data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a9f113",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Google DeepMind's Alphafold (2021)\n",
    "\n",
    "![alphafold](alphafold.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c444d276",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### OpenAI's ChatGPT and GPT-4 Models (2023):\n",
    "\n",
    "![gpt4 exams](gpt4_exams.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc30248e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML in Materials Science:\n",
    "\n",
    "![choudhary timeline](choudhary_timeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff8c60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of Machine Learning Problems\n",
    "\n",
    "Machine Learning Problems can be divided into three general categories:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d53a5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Supervised Learning**: A predictive model is provided with a labeled dataset with the goal of making predictions based on these labeled examples\n",
    "    * Examples: regression, classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3e9a4b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Unsupervised Learning**: A model is applied to unlabeled data with the goal of discovering trends, patterns, extracting features, or finding relationships between data.\n",
    "    * Examples: clustering, dimensionality reduction, anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac60fb4c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Reinforcement Learning**: An agent learns to interact with an environment in order to maximize its cumulative rewards.\n",
    "    * Examples: intelligent control, game-playing, sequential design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041ea20",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "**When can supervised learning be applied?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa1a665",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Problems where the available data contains many different labeled examples\n",
    "* Problems that involve finding a model that maps a set of features (inputs) to labels (outputs).\n",
    "\n",
    "* A supervised learning dataset consists of $(\\mathbf{x}, y)$ pairs:\n",
    "\n",
    "    * $\\mathbf{x}$ is a vector of features (model inputs)\n",
    "    * $y$ is a label to be predicted (the model output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7291e7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* $y$ values can be continuous scalars, vectors, or discrete classes. \n",
    "\n",
    "* Here, we will assume $\\mathbf{x}$ is a real vector and $y$ is a continuous real scalar (unless otherwise specficied)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c84ee6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What is the goal of supervised Learning?**\n",
    "\n",
    "* The goal is to learn a model that makes accurate predictions (denoted $\\hat{y}$) of $y$ based on a vector of features $\\mathbf{x}$.\n",
    "\n",
    "* We can think of a model as a function $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$\n",
    "    * $\\mathcal{X}$ is the space of all possible feature vectors $\\mathbf{x}$\n",
    "    * $\\mathcal{Y}$ is the space of all labels $y$.\n",
    " \n",
    "![function](supervised_model.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d18bbd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of Supervised Learning Problems:\n",
    "\n",
    "The type of a supervised learning problem depends on the type of value $y$ we are attempting to predict:\n",
    "\n",
    "* If $y$ is a continuous value, it is a **regression problem**\n",
    "\n",
    "* If $y$ can be a finite number of values, it is a **classification problem**\n",
    "\n",
    "* If $y$ is a continuous probability (between $0$ and $1$), it is a **logistic regression** problem*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb93ab9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*In some textbooks, logistic regression also refers to a specific kind of model that is used for predicting probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e596e41a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Validity\n",
    "\n",
    "* A model $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$ is _valid_ if it approximately maps every set of features in $\\mathcal{X}$ to the correct label $\\mathcal{Y}$:\n",
    "\n",
    "$$ \\hat{y} = f(\\mathbf{x}) \\approx y$$\n",
    "\n",
    "* This must hold for every label $y$ associated with every possible feature vector $\\mathbf{x}$ in $\\mathcal{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c573331",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model validity is a subjective property, because we may not know what the correct label $y$ is for every single value $\\mathbf{x}$ in $\\mathcal{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e819df",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Example: classifying images of cats vs. dogs\n",
    "\n",
    "![catdog](catdog.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da018be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Often, we only know the $(\\mathbf{x},y)$ pairs in our dataset.\n",
    "\n",
    "* If there is noise or bias in our data, even those $(\\mathbf{x},y)$ pairs may be unreliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38bd555",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Even if a model fits the dataset perfectly, we may not know if the fit is _valid_, because we don't know the $(\\mathbf{x},y)$ pairs that lie outside the training dataset:\n",
    "\n",
    "![supervised model ood](supervised_model_ood.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea749f51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing between two models:\n",
    "\n",
    "* Consider the following two models fit to the same dataset:\n",
    "\n",
    "![supervised model example](fit_examples.png)\n",
    "\n",
    "**Which model is the more valid model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95626646",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Even though the polynomial perfectly fits the data, it is not valid because it extrapolates poorly:\n",
    "\n",
    "![fit extrapolations](fit_extrapolation.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac243cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimating Validity:\n",
    "\n",
    "Here's how we can solve the problem of estimating model validity:\n",
    "\n",
    "* Purposely leave out a random subset of the data that the model is fit to.\n",
    "\n",
    "    * Use that subset to evaluate the accuracy of the fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc75ef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This subset that we leave out is called the _validation_ set.\n",
    "\n",
    "\n",
    "* The subset we fit the model to is called the _training set_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a54e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Traning Validation, and Test Sets:\n",
    "\n",
    "* Common practice is to set aside 10% of the data as the validation set.\n",
    "* In some problems another 10% of the data is set aside as the _test_ set.\n",
    "\n",
    "![supervised split](supervised_split.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee3d132",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Validation vs. Test Sets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdc7891",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The validation set is used for comparing the accuracy of different models or instances of the same model with different parameters.\n",
    "\n",
    "* The _test_ set is used to provide a final, unbiased estimate of the best model selected using the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4277667",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Evaluating the final model accuracy on the test set eliminates selection bias associated with the accuracies on the validation set.\n",
    "\n",
    "    * The more models that are compared using the validation set, the greater the need for the test set.\n",
    "    \n",
    "    * This is especially true if you are reporting the statistical significance of your model's accuracy being better than another model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a173a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparing Data:\n",
    "\n",
    "* For each feature vector $\\mathbf{x}$, some features vary much more than other features.\n",
    "\n",
    "* To avoid making our model more sensitive to features with high variance, we _normalize_ each feature, so that it lies roughly on the interval $[-2,2]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce4986b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Normalization is a transformation $\\mathbf{x} \\mapsto \\mathbf{z}$:\n",
    "\n",
    "$$z_i = \\frac{x_i - \\mu_i}{\\sigma_i}$$\n",
    "\n",
    "* $\\mu_i$ and $\\sigma_i$ are the mean and standard deviation of the $i$th feature in the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ddba36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss functions:\n",
    "\n",
    "* To evaluate the accuracy of a model on a dataset, we use a _loss function_.\n",
    "\n",
    "* A loss function is function of a prediction $\\hat{y}$ and a true label $y$ that increases as the prediction deviates from the true label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218a5838",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Example (_square error loss_):\n",
    "\n",
    "$$E(\\hat{y}, y) = (\\hat{y} - y)^2$$\n",
    "\n",
    "* A good loss function should attain its minimum when $\\hat{y} = y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d35676c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Loss Functions:\n",
    "\n",
    "* We can evaluate how well a model $f$ fits a dataset $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$ by taking the average of a loss function evaluated on all $(\\mathbf{x}_i, y_i)$ pairs.\n",
    "\n",
    "_Examples:_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac2685f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Mean Square Error (MSE):\n",
    "\n",
    "    $$\\mathcal{E}(f) = \\frac{1}{N} \\sum_{n=1}^N (f(\\mathbf{x}_n) - y_n)^2$$\n",
    "\n",
    "* Mean Absolute Error (MAE):\n",
    "\n",
    "    $$\\mathcal{E}(f) = \\frac{1}{N} \\sum_{n=1}^N |f(\\mathbf{x}_n) - y_n|$$\n",
    "\n",
    "* Classification Accuracy:\n",
    "\n",
    "   $$\\mathcal{E}(f) = \\frac{1}{N} \\sum_{n=1}^N \\delta(\\hat{y} - y) = \\left[ \\frac{\\text{# Correct}}{\\text{Total}} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e33ad14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting Models to Data:\n",
    "\n",
    "* Most models have weights that must be adjusted to fit the training dataset:\n",
    "\n",
    "* Example (_1D polynomial regression_):\n",
    "    \n",
    "    $$f(x) = \\sum_{d=0}^{D} w_dx^d$$\n",
    "    \n",
    "* There are many different methods that can be used to find the optimial weights $w_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b0d78",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* The most common method for fitting the data is through _gradient descent_.\n",
    "\n",
    "    * Gradient descent makes iterative adjustments to weight values such that each adjustment decreases the model loss $\\mathcal{E}(f)$.\n",
    "\n",
    "    \n",
    "* Some models (such as linear regression) have optimal weights that can be solved for in closed form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc8414c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "* Gradient descent makes iterative adjustments to the model weights $\\mathbf{w}$:\n",
    "\n",
    "$$\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\nabla_w \\mathcal{E}(f)$$\n",
    "\n",
    "![gradient descent](grad_descent.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe535326",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Review: The Gradient\n",
    "\n",
    "The gradient of a function $g: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is the vector-valued function:\n",
    "\n",
    "$$\\nabla g(\\mathbf{w}) = \\begin{bmatrix}\n",
    "\\frac{\\partial g}{\\partial w_0}(\\mathbf{w}) & \\frac{\\partial g}{\\partial w_1}(\\mathbf{w}) & \\dots & \\frac{\\partial g}{\\partial w_n}(\\mathbf{w})\n",
    "\\end{bmatrix}^T$$\n",
    "\n",
    "* ($\\nabla g(\\mathbf{w})$ is a function $\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$)\n",
    "* The gradient at a point $\\mathbf{w}$ \"points\" in the direction in which $g$ increases the most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87905111",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tutorial: Classifying Perovskites\n",
    "\n",
    "* We will work with some basic classification models that classify perovskite materials as \"cubic\" or \"non-cubic\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4261f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recommended Reading:\n",
    "* Advanced Regression Models\n",
    "\n",
    "If possible, try to do the exercises.\n",
    "Bring your questions to our next meeting (next Monday)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
