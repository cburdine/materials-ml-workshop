{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f8e3e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Materials+ML Workshop Day 7\n",
    "\n",
    "![logo](logo.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea039bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Content for today:\n",
    "\n",
    "* Supervised Learning Review\n",
    "    * Regression, Logistic Regression, Classification\n",
    "    * Train/validation/Test sets\n",
    "    \n",
    "* Regression Models\n",
    "    * Linear Regression\n",
    "    * High-dimensional Embeddings\n",
    "    * Kernel Machines (if time)\n",
    "    \n",
    "* Application: Predicting Material Bandgaps\n",
    "    * Applying Regression Models (if time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ccbbf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Background Survey\n",
    "\n",
    "![background survey](survey_qr.png)\n",
    "\n",
    "## [https://forms.gle/FEWqPavJJYC9VzfH7](https://forms.gle/FEWqPavJJYC9VzfH7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b71a1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## The Workshop Online Book:\n",
    "\n",
    "### [https://cburdine.github.io/materials-ml-workshop/](https://cburdine.github.io/materials-ml-workshop/)\n",
    "\n",
    "* **Recordings of Previous Workshop Sessions (and slides) now available!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3466e435",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tentative Workshop Schedule:\n",
    "\n",
    "| Session       | Date          | Content                                                       |\n",
    "| -------       | ----          | -------                                                       |\n",
    "| Day 0         | 06/16/2023 (2:30-3:30 PM) | Introduction, Setting up your Python Notebook     |\n",
    "| Day 1         | 06/19/2023 (2:30-3:30 PM) | Python Data Types                                 |\n",
    "| Day 2         | 06/20/2023 (2:30-3:30 PM) | Python Functions and Classes                      |\n",
    "| Day 3     | 06/21/2023 (2:30-3:30 PM) | Scientific Computing with Numpy and Scipy         |\n",
    "| Day 4         | 06/22/2023 (2:30-3:30 PM) | Data Manipulation and Visualization              |\n",
    "| Day 5       | 06/23/2023 (2:30-3:30 PM) | Materials Science Packages                     |\n",
    "| Day 6       | 06/26/2023 (2:30-3:30 PM) | Introduction to ML, Supervised Learning           |\n",
    "| **Day 7**         | **06/27/2023 (2:30-3:30 PM)** | **Regression Models**                                 |\n",
    "| Day 8         | 06/28/2023 (2:30-3:30 PM) | Unsupervised Learning                             |\n",
    "| Day 9         | 06/29/2023 (2:30-3:30 PM) | Neural Networks                                   |\n",
    "| Day 10        | 06/30/2023 (2:30-3:30 PM) | Advanced Applications in Materials Science        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572813e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions\n",
    "\n",
    "* Intro to ML Content:\n",
    "    * Statistics Review\n",
    "    * Linear Algebra Review\n",
    "* Supervised Learning\n",
    "    * Models and validity\n",
    "    * Training, validation, and test sets\n",
    "    * Normalizing Data\n",
    "    * Gradient Descent\n",
    "    * Classification Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff8c60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of Machine Learning Problems\n",
    "\n",
    "Machine Learning Problems can be divided into three general categories:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d53a5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Supervised Learning**: A predictive model is provided with a labeled dataset with the goal of making predictions based on these labeled examples\n",
    "    * Examples: regression, classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3e9a4b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Unsupervised Learning**: A model is applied to unlabeled data with the goal of discovering trends, patterns, extracting features, or finding relationships between data.\n",
    "    * Examples: clustering, dimensionality reduction, anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac60fb4c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Reinforcement Learning**: An agent learns to interact with an environment in order to maximize its cumulative rewards.\n",
    "    * Examples: intelligent control, game-playing, sequential design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c84ee6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Learning\n",
    "\n",
    "* Learn a model that makes accurate predictions $\\hat{y}$ of $y$ based on a vector of features $\\mathbf{x}$.\n",
    "\n",
    "* We can think of a model as a function $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$\n",
    "    * $\\mathcal{X}$ is the space of all possible feature vectors $\\mathbf{x}$\n",
    "    * $\\mathcal{Y}$ is the space of all labels $y$.\n",
    " \n",
    "![function](supervised_model.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38bd555",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problems with Model Validity\n",
    "\n",
    "* Even if a model fits the dataset perfectly, we may not know if the fit is _valid_, because we don't know the $(\\mathbf{x},y)$ pairs that lie outside the training dataset:\n",
    "\n",
    "![supervised model ood](supervised_model_ood.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a54e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Traning Validation, and Test Sets:\n",
    "\n",
    "* Common practice is to set aside 10% of the data as the validation set.\n",
    "* In some problems another 10% of the data is set aside as the _test_ set.\n",
    "\n",
    "![supervised split](supervised_split.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee3d132",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Validation vs. Test Sets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdc7891",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The _validation set_ is used for comparing the accuracy of different models or instances of the same model with different parameters.\n",
    "\n",
    "* The _test_ set is used to provide a final, unbiased estimate of the best model selected using the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a173a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparing Data:\n",
    "\n",
    "* To avoid making our model more sensitive to features with high variance, we _normalize_ each feature, so that it lies roughly on the interval $[-2,2]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce4986b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Normalization is a transformation $\\mathbf{x} \\mapsto \\mathbf{z}$:\n",
    "\n",
    "$$z_i = \\frac{x_i - \\mu_i}{\\sigma_i}$$\n",
    "\n",
    "* $\\mu_i$ and $\\sigma_i$ are the mean and standard deviation of the $i$th feature in the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d35676c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Loss Functions:\n",
    "\n",
    "* We can evaluate how well a model $f$ fits a dataset $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$ by taking the average of a loss function evaluated on all $(\\mathbf{x}_i, y_i)$ pairs.\n",
    "\n",
    "_Examples:_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac2685f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Mean Square Error (MSE):\n",
    "\n",
    "    $$\\mathcal{E}(f) = \\frac{1}{N} \\sum_{n=1}^N (f(\\mathbf{x}_n) - y_n)^2$$\n",
    "\n",
    "* Mean Absolute Error (MAE):\n",
    "\n",
    "    $$\\mathcal{E}(f) = \\frac{1}{N} \\sum_{n=1}^N |f(\\mathbf{x}_n) - y_n|$$\n",
    "\n",
    "* Classification Accuracy:\n",
    "\n",
    "   $$\\mathcal{E}(f) = \\frac{1}{N} \\sum_{n=1}^N \\delta(\\hat{y} - y) = \\left[ \\frac{\\text{# Correct}}{\\text{Total}} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc8414c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "* Gradient descent makes iterative adjustments to the model weights $\\mathbf{w}$:\n",
    "\n",
    "$$\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\nabla_w \\mathcal{E}(f)$$\n",
    "\n",
    "![gradient descent](grad_descent.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc1f37",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today's Content:\n",
    "\n",
    "**Advanced Regression Models**\n",
    "\n",
    "* Multivariate Linear Regression\n",
    "    * High-Dimensional Embeddings\n",
    "* Regularization\n",
    "    * Undervitting vs. overfitting\n",
    "    * Ridge regression\n",
    "* Kernel Machines (if time)\n",
    "    * Support Vectors\n",
    "    * Kernel Functions\n",
    "    * Support Vector Machines\n",
    "* Application:\n",
    "    * Predicting Bandgaps of Materials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df83f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multivariate Linear Regression\n",
    "\n",
    "* Multivariate Linear regression is a type of regression model that estimates a label as a linear combination of features:\n",
    "\n",
    "$$\\hat{y} = f(\\mathbf{x}) = w_0 + \\sum_{i=1}^D w_i x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d93bc4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If $\\mathbf{x}$ has $D$ features, there are $D+1$ weights we must determine to fit $f$ to data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f4983a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We can re-write the linear regression model in vector form:\n",
    "\n",
    "    * Let $\\underline{\\mathbf{x}} = \\begin{bmatrix} 1 & x_1 & x_2 & \\dots & x_D \\end{bmatrix}^T$ ($\\mathbf{x}$ padded with a 1)\n",
    "    * Let $\\mathbf{w} = \\begin{bmatrix} w_0 & w_1 & w_2 & \\dots & w_D \\end{bmatrix}^T$ (the weight vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d582cea9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $f(\\mathbf{x})$ is just the inner product (i.e. dot product) of these two vectors:\n",
    "\n",
    "$$\\hat{y} = f(\\mathbf{x}) = \\underline{\\mathbf{x}}^T\\mathbf{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300787ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* For these linear regression models, it is helpful to represent a dataset $\\{ (\\mathbf{x}_n,y_n) \\}_{n=1}^N$ as a matrix-vector pair $(\\mathbf{X},\\mathbf{y})$, given by:\n",
    "\n",
    "$$\\mathbf{X} = \\begin{bmatrix} \n",
    "\\underline{\\mathbf{x}_1}^T \\\\\n",
    "\\underline{\\mathbf{x}_2}^T \\\\\n",
    "\\vdots \\\\\n",
    "\\underline{\\mathbf{x}_N}^T\n",
    "\\end{bmatrix},\\qquad\\qquad \\mathbf{y} = \\begin{bmatrix}\n",
    "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b0ef8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This is helpful because it allows us to write the MSE (mean square error) model loss function in matrix form:\n",
    "\n",
    "$$\\text{MSE}: \\mathcal{E}(f) = \\frac{1}{N} \\sum_{n=1}^N (\\hat{y} - y)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ea0cfe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In terms of $\\mathbf{X}$ and $\\mathbf{y}$, we write:\n",
    "\n",
    "$$\\mathcal{E}(f) = \\frac{1}{N}(\\mathbf{X}\\mathbf{w} -\\mathbf{y})^T(\\mathbf{X}\\mathbf{w} - \\mathbf{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dda1e02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* It can be shown that the weight vector $\\mathbf{w}$ minimizing the MSE $\\mathcal{E}(f)$ can be computed in closed form: \n",
    "\n",
    "$$\\mathbf{w} = \\mathbf{X}^+\\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd062e73",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Above, $\\mathbf{X}^+$ denotes the [Moore-Penrose inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) (sometimes called the _pseudo-inverse_) of $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b22b1b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If the dataset size $N$ is sufficiently large such that $\\mathbf{X}$ has linearly independent columns, the optimal weights can be computed as:\n",
    "\n",
    "$$\\mathbf{w} = \\mathbf{X}^{+}\\mathbf{y} = \\left( (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\right)\\mathbf{y}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073936cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## High-Dimensional Embeddings\n",
    "\n",
    "* Often, the trends of $y$ with respect to $\\mathbf{x}$ are non-linear, so multivariate linear regression may fail to give good results.\n",
    "\n",
    "* One way of handling this is by _embedding_ the data in a higher-dimensional space using many different non-linear functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66152efd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\phi_j(\\mathbf{x}) : \\mathbb{R}^{D} \\rightarrow \\mathbb{R}\\qquad (j = 1, 2, ..., D_{emb})$$\n",
    "\n",
    "(The $\\phi_j$ are nonlinear functions, and $D_{emb}$ is the embedding dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a3505",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* After embedding the data in a $D_{emb}$-dimensional space, we can apply linear regression on to the embedded data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2321134b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\hat{y} = f(\\mathbf{x}) = w_0 + \\sum_{j=1}^{D_{emb}} w_j \\phi_j(\\mathbf{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48c1c06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The loss function used in these models is also commonly the mean square error (MSE):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c93ee4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\mathcal{E}(f) = \\frac{1}{N}(\\mathbf{\\Phi}(\\mathbf{X})\\mathbf{w} - \\mathbf{y})^T(\\mathbf{\\Phi}(\\mathbf{X})\\mathbf{w} - \\mathbf{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19668bad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Above, the quantity $\\Phi(\\mathbf{X})$ is the embedding of the data matrix $\\mathbf{X}$. It is a matrix with the following form:\n",
    "\n",
    "$$\\mathbf{\\Phi}(\\mathbf{X}) = \\begin{bmatrix}\n",
    "1 & \\phi_1(\\mathbf{x}_1) & \\phi_2(\\mathbf{x}_1) & \\dots  & \\phi_{D_{emb}}(\\mathbf{x}_1) \\\\\n",
    "1 & \\phi_1(\\mathbf{x}_2) & \\phi_2(\\mathbf{x}_2) & \\dots  & \\phi_{D_{emb}}(\\mathbf{x}_2) \\\\\n",
    "\\vdots & \\vdots               & \\vdots               & \\ddots & \\vdots \\\\\n",
    "1 & \\phi_1(\\mathbf{x}_N) & \\phi_2(\\mathbf{x}_N) & \\dots  & \\phi_{D_{emb}}(\\mathbf{x}_N)\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f0d01",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Fitting a linear regression model in a high-dimensional space can be computationally expensive:\n",
    "    \n",
    "$$\\hat{y} = f(\\mathbf{x}) = w_0 + \\sum_{j=1}^{D_{emb}} w_j \\phi_j(\\mathbf{x})$$\n",
    "\n",
    "$$\\mathbf{w} = \\Phi(\\mathbf{X})^+\\mathbf{y}$$\n",
    "\n",
    "* This is especially true if $D_{emb} \\gg D$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c381133",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Fitting polynomials:\n",
    "\n",
    "* To fit a polynomial to 1D $(x_i, y_i)$ data, we can use the following embedding matrix:\n",
    "\n",
    "$$\\mathbf{\\Phi}(\\mathbf{X}) = \\begin{bmatrix}\n",
    "1 & x_1 &  x_1^2 & \\dots & x_1^{D_{emb}} \\\\\n",
    "1 & x_2 &  x_2^2 & \\dots & x_2^{D_{emb}} \\\\\n",
    "1 & x_3 &  x_3^2 & \\dots & x_3^{D_{emb}} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_N & x_N^2 & \\dots & x_N^{D_{emb}}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "* This matrix is referred to as a _Vandermonde_ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50980e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Underfitting and Overfitting\n",
    "\n",
    "* High-dimensional embeddings are powerful because they give a model enough degrees of freedom to conform to non-linearities in the data.\n",
    "\n",
    "* The more degrees of freedom a model has the more prone it is to \"memorizing\" the data instead of \"learning from it\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6136c68c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Fitting a model requires striking a balance between these two extremes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d9e9f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* A model _underfits_ the data if it has insufficient degrees of freedom to model the data.\n",
    "    * Underfitting often results from poor model choice.\n",
    "    * When underfitting occurs, both the training and validation error are very high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e06053",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A model _overfits_ the data if it has too many degrees of freedom such that it fails to generalize well outside of the training data.\n",
    "    * Overfitting results from applying a model that is too complex to a dataset that is too small. \n",
    "    * When overfitting occurs, the training error plateaus at a minimum (typically at zero) and the validation error increases suddenly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e38341",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Polynomial Regression\n",
    "\n",
    "\n",
    "![poly_regression](poly_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cec6f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* We can diagnose underfitting and overfitting by evaluating the training and validation error as a function of model complexity (in this case, $D_{emb}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32ae770",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Polynomial Regression Example:**\n",
    "\n",
    "![poly fits](poly_regression_fits.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac477dc2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization:\n",
    "\n",
    "* One way of reducing overfitting is by gathering more data.\n",
    "\n",
    "    * Having more data makes it harder for a model to \"memorize\" the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20510e3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Another way to reduce overfitting is to apply _regularization_\n",
    "\n",
    "    * Regularization refers to the use of some mechanism that deliberately reduces the flexibility of a model in order to reduce the validation set error\n",
    "    \n",
    "    * A common form of regularization is penalizing the model for having large weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c449dcf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* For most models, a penalty term is added to the overall model loss function.\n",
    "    * The model minimizes the loss while not incurring too large of a penalty:\n",
    "    \n",
    "    $$\\text{ Penalty Term } = \\lambda \\sum_{j} w_j^2 = \\lambda(\\mathbf{w}^T\\mathbf{w})$$\n",
    "\n",
    "* The parameter $\\lambda$ is called the _regularization parameter_\n",
    "    * as $\\lambda$ increases, more regularization is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fbbd54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge Regression\n",
    "\n",
    "* _Ridge Regression_ is a form of regression directly adds this regularization term to the MSE:\n",
    "\n",
    "$$\\mathcal{E}(f) = \\frac{1}{N}(\\mathbf{\\Phi}(\\mathbf{X})\\mathbf{w} - \\mathbf{y})^T(\\mathbf{\\Phi}(\\mathbf{X})\\mathbf{w} - \\mathbf{y}) + \\underbrace{\\lambda(\\mathbf{w}^T\\mathbf{w})}_{\\text{regularization term}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f11eb3a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* For any value of $\\lambda$ the optimal weights $\\mathbf{w}$ for a ridge regression problem can be computed in closed form:\n",
    "\n",
    "$$\\mathbf{w} = \\left((\\mathbf{\\Phi}(\\mathbf{X})^T\\mathbf{\\Phi}(\\mathbf{X}) + \\lambda\\mathbf{I})^{-1} \\mathbf{\\Phi}(\\mathbf{X})^T \\right) \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a762cf6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernel Machines\n",
    "\n",
    "* Kernel machines are an equivalent form of high-dimensional embedding models that avoid computing an embedding entirely:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b096031",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ f(\\mathbf{x}) = w_0 + \\sum_{i=1}^{D_{emb}} w_i\\phi_i(\\mathbf{x})\\quad \\Rightarrow \\quad f(\\mathbf{x}) = w_0 + \\sum_{n=1}^N (\\alpha_n - \\alpha_n^*)K(\\mathbf{x}_n,\\mathbf{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522da13c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Instead of enbedding data directly, kernel machines compute only the inner products of pairs of data points in the embedding space.\n",
    "\n",
    "* This inner product is computed by a _kernel function $K(\\mathbf{x}, \\mathbf{x}')$.\n",
    "\n",
    "* Kernel machines even allow us to perform linear regression in _infinite dimensional spaces_!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f18062",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tutorial: Bandgap Prediction\n",
    "\n",
    "* We will work with some data obtained from the Materials Project database to develop a model that predicts the bandgap of materials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4261f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recommended Reading:\n",
    "\n",
    "* Unsupervised Learning\n",
    "\n",
    "If possible, try to do the exercises.\n",
    "Bring your questions to our next meeting tomorrow."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
