{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f8e3e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Materials+ML Workshop Day 8\n",
    "\n",
    "![logo](logo.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea039bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Content for today:\n",
    "\n",
    "* Regression Models Review\n",
    "    * Linear Regression\n",
    "    * High-dimensional Embeddings\n",
    "    * Kernel Machines\n",
    "    \n",
    "* Unsupervised Learning\n",
    "    * Feature Selection\n",
    "    * Dimensionality reduction\n",
    "    * Clustering\n",
    "    * Distribution Estimation\n",
    "    * Anomaly Detection\n",
    "    \n",
    "* Application: Classifying Superconductors\n",
    "    * Application of unsupervised methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ccbbf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Background Survey\n",
    "\n",
    "![background survey](survey_qr.png)\n",
    "\n",
    "## [https://forms.gle/FEWqPavJJYC9VzfH7](https://forms.gle/FEWqPavJJYC9VzfH7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b71a1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## The Workshop Online Book:\n",
    "\n",
    "### [https://cburdine.github.io/materials-ml-workshop/](https://cburdine.github.io/materials-ml-workshop/)\n",
    "\n",
    "* **Recordings of Previous Workshop Sessions (and slides) now available!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3466e435",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tentative Workshop Schedule:\n",
    "\n",
    "| Session       | Date          | Content                                                       |\n",
    "| -------       | ----          | -------                                                       |\n",
    "| Day 0         | 06/16/2023 (2:30-3:30 PM) | Introduction, Setting up your Python Notebook     |\n",
    "| Day 1         | 06/19/2023 (2:30-3:30 PM) | Python Data Types                                 |\n",
    "| Day 2         | 06/20/2023 (2:30-3:30 PM) | Python Functions and Classes                      |\n",
    "| Day 3     | 06/21/2023 (2:30-3:30 PM) | Scientific Computing with Numpy and Scipy         |\n",
    "| Day 4         | 06/22/2023 (2:30-3:30 PM) | Data Manipulation and Visualization              |\n",
    "| Day 5       | 06/23/2023 (2:30-3:30 PM) | Materials Science Packages                     |\n",
    "| Day 6       | 06/26/2023 (2:30-3:30 PM) | Introduction to ML, Supervised Learning           |\n",
    "| Day 7         | 06/27/2023 (2:30-3:30 PM) | Regression Models                               |\n",
    "| **Day 8**         | **06/28/2023 (2:30-3:30 PM)** | **Unsupervised Learning**                             |\n",
    "| Day 9         | 06/29/2023 (2:30-3:30 PM) | Neural Networks                                   |\n",
    "| Day 10        | 06/30/2023 (2:30-3:30 PM) | Advanced Applications in Materials Science        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572813e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions\n",
    "\n",
    "* Regression Models\n",
    "    * Linear Regression\n",
    "    * High-dimensional Embeddings\n",
    "    * Kernel Machines\n",
    "    * Supervised Learning (in general)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df83f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multivariate Linear Regression\n",
    "\n",
    "* Multivariate Linear regression is a type of regression model that estimates a label as a linear combination of features:\n",
    "\n",
    "$$\\hat{y} = f(\\mathbf{x}) = w_0 + \\sum_{i=1}^D w_i x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f4983a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We can re-write the linear regression model in vector form:\n",
    "\n",
    "    * Let $\\underline{\\mathbf{x}} = \\begin{bmatrix} 1 & x_1 & x_2 & \\dots & x_D \\end{bmatrix}^T$ ($\\mathbf{x}$ padded with a 1)\n",
    "    * Let $\\mathbf{w} = \\begin{bmatrix} w_0 & w_1 & w_2 & \\dots & w_D \\end{bmatrix}^T$ (the weight vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d582cea9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $f(\\mathbf{x})$ is just the inner product (i.e. dot product) of these two vectors:\n",
    "\n",
    "$$\\hat{y} = f(\\mathbf{x}) = \\underline{\\mathbf{x}}^T\\mathbf{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dda1e02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Closed Form Solution:\n",
    "\n",
    "* Multivariate Linear Regression:\n",
    "\n",
    "$$\\mathbf{w} = \\mathbf{X}^+\\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd062e73",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Above, $\\mathbf{X}^+$ denotes the [Moore-Penrose inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) (sometimes called the _pseudo-inverse_) of $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b22b1b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If the dataset size $N$ is sufficiently large such that $\\mathbf{X}$ has linearly independent columns, the optimal weights can be computed as:\n",
    "\n",
    "$$\\mathbf{w} = \\mathbf{X}^{+}\\mathbf{y} = \\left( (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\right)\\mathbf{y}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073936cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## High-Dimensional Embeddings\n",
    "\n",
    "* Often, the trends of $y$ with respect to $\\mathbf{x}$ are non-linear, so multivariate linear regression may fail to give good results.\n",
    "\n",
    "* One way of handling this is by _embedding_ the data in a higher-dimensional space using many different non-linear functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66152efd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\phi_j(\\mathbf{x}) : \\mathbb{R}^{D} \\rightarrow \\mathbb{R}\\qquad (j = 1, 2, ..., D_{emb})$$\n",
    "\n",
    "(The $\\phi_j$ are nonlinear functions, and $D_{emb}$ is the embedding dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2321134b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\hat{y} = f(\\mathbf{x}) = w_0 + \\sum_{j=1}^{D_{emb}} w_j \\phi_j(\\mathbf{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50980e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Underfitting and Overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6136c68c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Finding the best fit of a model requires striking a balance between underfitting and overfitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d9e9f0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A model _underfits_ the data if it has insufficient degrees of freedom to model the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e06053",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A model _overfits_ the data if it has too many degrees of freedom such that it fails to generalize well outside of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32ae770",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Polynomial Regression Example:**\n",
    "\n",
    "![poly fits](poly_regression_fits.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac477dc2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization:\n",
    "\n",
    "* To reduce overfitting, we apply _regularization_:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c449dcf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Usually, a penalty term is added to the overall model loss function:\n",
    "    \n",
    "    $$\\text{ Penalty Term } = \\lambda \\sum_{j} w_j^2 = \\lambda(\\mathbf{w}^T\\mathbf{w})$$\n",
    "\n",
    "* The parameter $\\lambda$ is called the _regularization parameter_\n",
    "    * as $\\lambda$ increases, more regularization is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0890e176",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today's Content:\n",
    "\n",
    "**Unsupervised Learning**\n",
    "\n",
    "* Feature Selection\n",
    "* Dimensionality reduction\n",
    "* Clustering\n",
    "* Distribution Estimation\n",
    "* Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82158c48",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Learning Models:\n",
    "\n",
    "* Models applied to unlabeled data with the goal of discovering trends, patterns, extracting features, or finding relationships between data.\n",
    "    * Deals with datasets of _features only_\n",
    "    * (just $\\mathbf{x}$, not $(\\mathbf{x},y)$ pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e74e48",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![unsupervised learning](unsupervised_learning.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad82b77",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Selection and Dimensionality Reduction\n",
    "\n",
    "* Determines which features are the most \"meaningful\" in explaining how the data is distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e65abd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Sometimes we work with high-dimensional data that is very sparse\n",
    "\n",
    "* Reducing the dimensionality of the data might be necessary\n",
    "    * Reduces computational complexity\n",
    "    * Eliminates unnecessary (or redundant) features\n",
    "    * Can even improve model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7072d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Importance of Dimensionality\n",
    "\n",
    "* Dimensionality is an important concept in materials science.\n",
    "    * The dimensionality of a material affects its properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa769749",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Much like materials, the dimensionality of a dataset can say a lot about the properties of a dataset:\n",
    "    * How complex is the data?\n",
    "    * Does the data have fewer degrees of freedom than features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe93842",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Sometimes, data can be confined to some low-dimensional manifold embedded in a higher-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669549cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example: The \"Swiss Roll\" manifold**\n",
    "\n",
    "![Swiss roll](swiss_roll.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd9d1b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: The Covariance Matrix\n",
    "\n",
    "* The Covariance Matrix describes the variance of data in more than one dimension:\n",
    "\n",
    "$$\\mathbf{\\Sigma} = \\begin{bmatrix}\n",
    "\\sigma_{1}^2 & \\sigma_{12} & \\dots & \\sigma_{1d} \\\\\n",
    "\\sigma_{21} & \\sigma_{2}^2 & \\dots & \\sigma_{2d} \\\\\n",
    "\\vdots      & \\vdots      & \\ddots & \\vdots \\\\\n",
    "\\sigma_{d1} & \\sigma_{d2} & \\dots & \\sigma_{d}^2\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "* $\\Sigma_{ii} = \\sigma_i^2$: variance in dimension $i$\n",
    "* $\\Sigma_{ij} = \\sigma_{ij}$: covariance between dimensions $i$ and $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3724d7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\Sigma_{ij} = \\frac{1}{N} \\sum_{n=1}^N ((\\mathbf{x}_n)_i - \\mu_i)((\\mathbf{x}_n)_j - \\mu_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf1e1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Correlation Matrix:\n",
    "\n",
    "* Recall that it is generally a good idea to normalize our data:\n",
    "\n",
    "$$\\mathbf{x} \\mapsto \\mathbf{z}:\\quad z_i = \\frac{x_i - \\mu_i}{\\sigma_i}$$\n",
    "\n",
    "* The _correlation matrix_ (denoted $\\bar{\\Sigma}$) is the covariance matrix of the normalized data:\n",
    "\n",
    "$$ \\bar{\\Sigma} = \\frac{1}{N} \\sum_{n=1}^N \\mathbf{z}_n\\mathbf{z}_n^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20237d6a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The entries of the correlation matrix (in terms of the original data) are:\n",
    "\n",
    "$$\\bar{\\Sigma}_{ij} = \\frac{1}{N} \\sum_{n=1}^N \\frac{((\\mathbf{x}_n)_i - \\mu_i)((\\mathbf{x}_n)_j - \\mu_j)}{\\sigma_i\\sigma_j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b58e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpreting the Correlation Matrix\n",
    "\n",
    "$$\\bar{\\Sigma}_{ij} = \\frac{1}{N} \\sum_{n=1}^N \\frac{((\\mathbf{x}_n)_i - \\mu_i)((\\mathbf{x}_n)_j - \\mu_j)}{\\sigma_i\\sigma_j}$$\n",
    "\n",
    "* The diagonal of the correlation matrix consists of $1$s. (Why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da807a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The off-diagonal components describe the strength of correlation between feature dimensions $i$ and $j$\n",
    "    * Positive values: positive correlation\n",
    "    * Negative values: negative correlation\n",
    "    * Zero values:     no correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28159c9b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Components Analysis (PCA)\n",
    "\n",
    "* The eigenvectors of the correlation matrix are called _principal components_.\n",
    "\n",
    "* The associated eigenvalues describe the proportion of the data variance in the direction of each principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62623b77",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\bar{\\Sigma} = P D P^{T}$$\n",
    "\n",
    "* $D$: Diagonal matrix (eigenvalues along diagonal)\n",
    "* $P$: Principal component matrix (columns are principal components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d13df",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Since $\\bar{\\Sigma}$ is symmetric, the principal components are all orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454f6aba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dimension reduction with PCA\n",
    "\n",
    "We can project our (normalized) data onto the first $n$ principal components to reduce the dimensionality of the data, while still keeping most of the variance:\n",
    "\n",
    "$$\\mathbf{z} \\mapsto \\mathbf{u} = \\begin{bmatrix}\n",
    "\\mathbf{z}^T\\mathbf{p}^{(1)} \\\\\n",
    "\\mathbf{z}^T\\mathbf{p}^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{z}^T\\mathbf{p}^{(n)} \\\\\n",
    "\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b4e42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering and Distribution Estimation\n",
    "\n",
    "* Clustering methods allow us to identify dense groupings of data.\n",
    "\n",
    "* Distribution Estimation allows us to estimate the probability distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e37ebf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Means Clustering:\n",
    "\n",
    "* $k$-means is a popular clustering algorithm that identifies the centerpoints  a specified number of clusters $k$\n",
    "\n",
    "* These center points are called _centroids_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7c9d2b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![kmeans](kmeans.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59905723",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernel Density Estimation:\n",
    "\n",
    "* Kernel Density Estimation (KDE) estimates the probability distribution of an entire dataset\n",
    "\n",
    "* Estimates the distribution as a sum of multivariate normal \"bumps\" at the position of each datapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c2219e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![kde](kernel_density_estimation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79377856",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Mixture Model\n",
    "\n",
    "* A Gaussian Mixture Model (GMM) performs both clustering and distribution estimation simultaneously.\n",
    "\n",
    "* Works by fitting a mixture of multivariate normal distributions to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778fab58",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![gmm](gaussian_mixture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be3e589",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application: Classifying Superconductors\n",
    "\n",
    "* Exploring the distribution of superconducting materials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4261f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recommended Reading:\n",
    "\n",
    "* Neural Networks \n",
    "\n",
    "(Note: some sections are still in progress ☹️)\n",
    "\n",
    "If possible, try to do the exercises.\n",
    "Bring your questions to our next meeting tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc30e0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
