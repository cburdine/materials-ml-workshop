

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Supervised Learning &#8212; Materials + Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'supervised_learning/supervised_learning';</script>
    <link rel="shortcut icon" href="../_static/logo_small.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Fitting Supervised Models" href="fitting_models.html" />
    <link rel="prev" title="Mathematics Review" href="../ml_intro/math_review.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.svg" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.svg" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Materials + ML Workshop
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../intro/intro.html">Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../intro/jupyter.html">Installing Python and Jupyter Notebook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/colab.html">Using Google Colab</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../python/getting_started.html">Getting Started with Python</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../python/python_basics.html">Python Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python/python_logic.html">Logic and Flow Control</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python/loops.html">Loops</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../data_types/python_data_types.html">Python Data Types</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../data_types/sets_and_dicts.html">Sets and Dictionaries</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../functions/functions_classes.html">Python Functions and Classes</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../functions/classes.html">Classes and Object-Oriented Programming</a></li>




</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../scicomp/numpy_scipy.html">Scientific Computing with Numpy and Scipy</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../scicomp/numpy.html">The Numpy Package</a></li>
<li class="toctree-l2"><a class="reference internal" href="../scicomp/scipy.html">The Scipy Package</a></li>



</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../data_vis/data_handling.html">Data Handling and Visualization</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../data_vis/data_vis.html">Data Visualization with Matplotlib</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../matsci/matsci_packages.html">Materials Science Python Packages</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../matsci/ase.html">ASE - The Atomic Simulation Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../matsci/pymatgen_MPRester.html">Pymatgen and the Materials Project API</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ml_intro/ml_intro.html">Introduction to Machine Learning</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ml_intro/statistics_review.html">Statistics Review</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml_intro/math_review.html">Mathematics Review</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">Supervised Learning</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="fitting_models.html">Fitting Supervised Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="basic_models.html">Application: Classifying Perovskites</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../regression/regression.html">Advanced Regression Models</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../regression/kernel_machines.html">Kernel Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../regression/regression_models.html">Application: Bandgap Prediction</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../unsupervised_learning/unsupervised_learning.html">Unsupervised Learning</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../unsupervised_learning/feature_selection.html">Feature Selection and Dimensionality Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unsupervised_learning/clustering.html">Clustering and Distribution Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unsupervised_learning/unsupervised_applications.html">Application: Classifying Superconductors</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../neural_networks/neural_networks.html">Neural Networks</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../neural_networks/training.html">Training Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neural_networks/architectures.html">Types of Neural Network Models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../applications/applications.html">Applications of ML to Materials Science</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/cburdine/materials-ml-workshop" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/cburdine/materials-ml-workshop/issues/new?title=Issue%20on%20page%20%2Fsupervised_learning/supervised_learning.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/supervised_learning/supervised_learning.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Supervised Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#obtaining-data">Obtaining Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#handling-data">Handling Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-training-set">The Training Set</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-validation-set">The Validation Set</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-test-set">The Test Set</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalizing-data">Normalizing Data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-selection">Model Selection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions">Solutions:</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-training-validation-and-test-sets">Exercise 1: Training, Validation, and Test Sets</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-polynomial-models">Exercise 2: Polynomial Models</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="supervised-learning">
<h1>Supervised Learning<a class="headerlink" href="#supervised-learning" title="Permalink to this heading">#</a></h1>
<p>Now that we have reviewed some of the necessary background material, we will will begin examining the most common type of machine learning problem: <em>supervised learning</em>. Supervised learning is applied to problems where the available data contains many different labeled examples, and the problem involves finding a model that maps a set of features (inputs) to labels (outputs).</p>
<p>Although data can take many different forms (i.e. numbers, vectors, images, text, 3D structures, etc.), we can think of a dataset as a collection of of <span class="math notranslate nohighlight">\((\mathbf{x}, y)\)</span> pairs, where <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is a set of features, and <span class="math notranslate nohighlight">\(y\)</span> is the label to be predicted. For now, we will consider the simplest case where <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is a vector of floating-point numbers and <span class="math notranslate nohighlight">\(y\)</span> is either (a) one of a finite number of mutually classes, or (b) a scalar quantity. In case (a), the supervised learning problem is a <em>classification problem</em>, where we must learn a model that makes prediction <span class="math notranslate nohighlight">\(\hat{y}\)</span> of the class <span class="math notranslate nohighlight">\(y\)</span> associated with the set of features <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. In case (b), the supervised learning problem is a <em>regression problem</em>, where we must learn a model that produces an estimate <span class="math notranslate nohighlight">\(\hat{y}\)</span> of <span class="math notranslate nohighlight">\(y\)</span> based on <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<p>For both classification and regression problems, we can think of a model as a function <span class="math notranslate nohighlight">\(f: \mathcal{X} \rightarrow \mathcal{Y}\)</span> that maps the space of possible features <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> into the space of possible labels <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>.</p>
<p><img alt="Illustration of a Supervised Model" src="../_images/supervised_model.svg" /></p>
<p>Ideally, we would like the function <span class="math notranslate nohighlight">\(f\)</span> to be a <em>valid</em> model, meaning it maps every possible set of features in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> to the correct output label; however, finding such a function may be impossible for a number of reasons. First, it is possible that <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> might be an infinite set, meaning it is impossible to verify that <span class="math notranslate nohighlight">\(f\)</span> is valid for all sets of features <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. In some situations, we may not even know what the correct labels are for every single <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. For these reasons, it might appear that <em>learning</em> a valid model <span class="math notranslate nohighlight">\(f: \mathcal{X} \rightarrow \mathcal{Y}\)</span> is an impossible challenge.</p>
<p>Fortunately, we have a powerful tool that we can use to tackle this seemingly insurmountable task: <em>data</em>. Using the data we have gathered, we can estimate the validity of a model by determining how well it <em>fits</em> the data. Quantitatively speaking, we say that a model <span class="math notranslate nohighlight">\(f\)</span> fits a dataset of <span class="math notranslate nohighlight">\((\mathbf{x}_i,y_i)\)</span> pairs if <span class="math notranslate nohighlight">\(f(\mathbf{x}_i) \approx y_i\)</span> for each <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>We also know that following statement is generally true with regards to our data and any potential model <span class="math notranslate nohighlight">\(f\)</span>:</p>
<blockquote>
<div><p>If a model <span class="math notranslate nohighlight">\(f : \mathcal{X} \rightarrow \mathcal{Y}\)</span> is valid, then it will fit the data.</p>
</div></blockquote>
<p>Indeed, any model that is valid should produce a good fit to the data, provided that enough features are accounted for in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and that sufficient care is taken to eliminate noise in the data.</p>
<p>Now, take a moment to consider the converse of the previous statement:</p>
<blockquote>
<div><p>If a model <span class="math notranslate nohighlight">\(f : \mathcal{X} \rightarrow \mathcal{Y}\)</span> fits the data, then it is valid.</p>
</div></blockquote>
<p>Is this statement also true? At first glance, you might be tempted into thinking it might be, but it is in general not true. In fact, sometimes a model that fits the data better than another model may actually be less valid. An an illustration of this, consider the following regression problem, where we have proposed two different fits:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># define data distribution:</span>
<span class="k">def</span> <span class="nf">y_distribution</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
	<span class="k">return</span> <span class="o">-</span><span class="mf">0.1</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.9</span> <span class="o">+</span> \
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span><span class="mf">0.3</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># the data we obtain will naturally fit the valid model with</span>
<span class="c1"># some noise due to experimental errors:</span>
<span class="n">data_n</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">data_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">data_n</span><span class="p">)</span>
<span class="n">data_y</span> <span class="o">=</span> <span class="n">y_distribution</span><span class="p">(</span><span class="n">data_x</span><span class="p">)</span>

<span class="c1"># fit data to a line (degree 1 polynomial):</span>
<span class="n">xy_linefit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">poly1d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span><span class="n">data_y</span><span class="p">,</span><span class="n">deg</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># fit data to N-1 degree polynomial to data:</span>
<span class="n">xy_polyfit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">poly1d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span><span class="n">data_y</span><span class="p">,</span><span class="n">deg</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">data_x</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># plot data and valid/invalid models for comparison:</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">eval_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span><span class="n">data_y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Data $(x,y)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">eval_x</span><span class="p">,</span> <span class="n">xy_linefit</span><span class="p">(</span><span class="n">eval_x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Linear Fit $f(x)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">eval_x</span><span class="p">,</span> <span class="n">xy_polyfit</span><span class="p">(</span><span class="n">eval_x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Polynomial Fit $f(x)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/1239d1fe99c5081ef7536a9a0296dae58e54d3ee6cbc52762a6340a1ece49388.png" src="../_images/1239d1fe99c5081ef7536a9a0296dae58e54d3ee6cbc52762a6340a1ece49388.png" />
</div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Since the data above is randomly generated, notebook output may vary.</p>
</div>
<p>Above, we have randomly generated data and proposed two models: a <em>linear fit</em> of the form <span class="math notranslate nohighlight">\(f(x) = mx + b\)</span> and a degree 11 polynomial fit of the form <span class="math notranslate nohighlight">\(f(x) = \sum_{n=0}^{11} a_n x^n\)</span>. Since a degree <span class="math notranslate nohighlight">\(N-1\)</span> polynomial can perfectly fit <span class="math notranslate nohighlight">\(N\)</span> data points, the polynomial fit exactly matches the data, whereas the linear fit does not. Nonetheless, it is likely that a linear fit is the more valid model, especially when we consider that the polynomial fit extrapolates very poorly outside of the interval <span class="math notranslate nohighlight">\([0,10]\)</span> that contained the dataset.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># determine the region of valid fits:</span>
<span class="n">extrap_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">valid_fit_region</span> <span class="o">=</span> <span class="p">[</span>
    <span class="o">-</span><span class="mf">0.1</span><span class="o">*</span><span class="n">extrap_x</span> <span class="o">+</span> <span class="mf">0.9</span> <span class="o">-</span> <span class="mf">0.3</span><span class="p">,</span>
    <span class="o">-</span><span class="mf">0.1</span><span class="o">*</span><span class="n">extrap_x</span> <span class="o">+</span> <span class="mf">0.9</span> <span class="o">+</span> <span class="mf">0.3</span>
<span class="p">]</span>

<span class="c1"># plot how well the fits extrapolate from the data:</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">extrap_x</span><span class="p">,</span> <span class="n">valid_fit_region</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">valid_fit_region</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                 <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Valid Fit Region&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">extrap_x</span><span class="p">,</span> <span class="n">xy_linefit</span><span class="p">(</span><span class="n">extrap_x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Linear Fit $f(x)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">extrap_x</span><span class="p">,</span> <span class="n">xy_polyfit</span><span class="p">(</span><span class="n">extrap_x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Polynomial Fit $f(x)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/77cbf99594f24a7aac77e801c77c0f7d911a3a833944583bc54a1270d4d2e656.png" src="../_images/77cbf99594f24a7aac77e801c77c0f7d911a3a833944583bc54a1270d4d2e656.png" />
</div>
</div>
<p>From this exercise, we see that some models are not valid, even though they may perfectly fit the data. So if we have a model that does seem to fit the data set well, how can we be sure that the model is also valid? Unfortunately, we can never know the degree to which a model is valid unless our dataset contains every possible input in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and its associated label. However, there are some general tactics we can employ to maximize the validity of our model. These are the three most important guidelines to keep in mind when working with supervised models:</p>
<ul class="simple">
<li><p><strong>Be cautious in how the data is obtained.</strong></p></li>
</ul>
<p>Is the data accurate? Are sources of noise identified and accounted for? Does the dataset adequately span the set of relevant input features <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and labels <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>? You can <em>never</em> have too much data, but you <em>can</em> have too much data of a particular kind.</p>
<ul class="simple">
<li><p><strong>Be cautious in how the data is handled.</strong></p></li>
</ul>
<p>How is the data handled to fit the model? Are training, validation, and test sets being used, and are they kept independent of one another? Are you avoiding selection bias? If you are enriching the dataset with additional features, are those features necessary and accurate?</p>
<ul class="simple">
<li><p><strong>Be sure that an appropriate model is being used.</strong></p></li>
</ul>
<p>Does the size of the dataset warrant the complexity of the model you are using? Are any symmetries of the data reflected in the symmetries of the model?</p>
<section id="obtaining-data">
<h2>Obtaining Data<a class="headerlink" href="#obtaining-data" title="Permalink to this heading">#</a></h2>
<p>Depending on the problem you are investigating, you may or may not play a role in the data collection process. Since we are materials scientists, the data we are working with often comes from either laboratory measurements or computational simulations. If you happen to have some control over the data collection processs, make every effort to ensure that the data is collected in a consistent manner, and that meaningful features and labels are reported accurately for each laboratory sample. If you are generating data through computational simulations, be sure that any data produced is at least consistent with values measured in the laboratory or with other independently reported values in the literature. After all, your aim is to develop a model of the physics and chemistry of the real world, not a model of the (often simplified) physics and chemistry of a simulation. As a famous computational scientist once said:</p>
<blockquote class="epigraph">
<div><p>“The purpose of computing is insight, not numbers.”</p>
<p class="attribution">—Richard Hamming</p>
</div></blockquote>
<p>If you are collecting data for a classification problem, make sure that all classes are represented in a balanced ratio, if possible. This will help a model avoid bias toward predicting the classes that appear more frequently in the dataset. Likewise, for regression problems be sure that the extremes of both <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> are contained in the dataset. This will help mitigate extrapolation error, as we saw in the previous example with the polynomial fit.</p>
</section>
<section id="handling-data">
<h2>Handling Data<a class="headerlink" href="#handling-data" title="Permalink to this heading">#</a></h2>
<p>Let’s return to the problem of determining model validity. We said earlier that a model <span class="math notranslate nohighlight">\(f\)</span> is <em>valid</em> if <span class="math notranslate nohighlight">\(f(\mathbf{x}) = \hat{y} \approx y\)</span> for points <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> both <em>inside</em> and <em>outside</em> of the dataset. However, we do not know what the correct values in <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> that correspond to feature sets <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span> lying outside the dataset:</p>
<p><img alt="Out-of-distribution Validity" src="../_images/supervised_model_ood.svg" /></p>
<p>Without data that is kept separate from the data that the model was fit to, we have no way of estimating the validity of the model, that is, how accurate the model is on <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> as a whole, not just on the dataset used to fit the model. This is why it is customary to set aside a subset of the data that is not used for fitting the model, but is used for only ensuring the validity of the model. This reserved subset of the dataset is usually referred to as the <em>validation set</em>. By measuring the model’s accuracy on the  <em>validation set</em>, we can effectively estimate how well the model generalizes to data that was not used. When using the validation set, we must be careful, since this estimate is only valid if the dataset provides sufficient coverage of the set of possible input features <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. In other words, any biases present in the dataset will result in biases in the estimates of model accuracy obtained by a validation set.</p>
<p>Today, it has become best practice in most supervised machine learning problems to split the dataset into <em>three</em> randomly selected independent subsets: a <em>training set</em>, a <em>validation set</em>, and a <em>test set</em>. Typically, these subsets comprise 80%, 10%, and 10% of the total dataset respectively:</p>
<p><img alt="Train-Validation-Test Split" src="../_images/supervised_split.svg" /></p>
<p>Let’s take a look at each of these three subsets and their importance in supervised learning:</p>
<section id="the-training-set">
<h3>The Training Set<a class="headerlink" href="#the-training-set" title="Permalink to this heading">#</a></h3>
<p>The training set comprises most of our dataset, and rightfully so. It contains the data that we fit our model to (often the process of fitting a model to data is referred to as <em>training</em>). Later in this workshop we will discuss some of the algorithms used to train various models.</p>
<p>As a general rule of thumb for large datasets, the training set should be about 80% of the data. For some smaller datasets you may want to instead use only 60-70% to ensure the validation and test sets are large enough.</p>
</section>
<section id="the-validation-set">
<h3>The Validation Set<a class="headerlink" href="#the-validation-set" title="Permalink to this heading">#</a></h3>
<p>The validation set is used to evaluate the accuracy of multiple configurations of the same model and to select the best one. Models can have many parameters that affect how flexible or inflexible the model is. For example, the degree of a polynomial fitted to one-dimensional data can be increased to make the model more flexible, but this flexibilty can reduce the model’s ability to make predictions outside of the training set. To find the optimal configuration of these parameters, multiple models are trained on the training set and their accuracies on the validation set are compared. As its name suggests, the validation set plays an important role in ensuring that the model is valid and can make predictions that generalize well outside the training set.</p>
</section>
<section id="the-test-set">
<h3>The Test Set<a class="headerlink" href="#the-test-set" title="Permalink to this heading">#</a></h3>
<p>Since the validation set is used to evaluate and select the best of many different models with different parameters, it is possible that the process of selecting the best model can introduce <a class="reference external" href="https://en.wikipedia.org/wiki/Selection_bias">selection bias</a> into the estimate that the validation set provides of the overall model’s performance. Selection bias can become increasingly problematic as the number of models evaluated on the same validation set increases. This is why we set aside a third subset of the data: the test set. The test set is used to provide a final unbiased evaluation of the model. It should never be used as a basis for comparing multiple models.</p>
</section>
<section id="normalizing-data">
<h3>Normalizing Data<a class="headerlink" href="#normalizing-data" title="Permalink to this heading">#</a></h3>
<p>When working with datasets containing many different features, the differences in the numerical scale of each feature can cause the model to be more sensitive to some features and less sensitive to others. Sometimes, even changing the units of the features can significantly affect the accuracy of a model. To avoid this problem and ensure that the accuracy of the model is invariant under how the data is scaled, we use a technique called <em>data normalization</em>.</p>
<p>Data normalization works as follows: instead of fitting a model to a training set consisting of <span class="math notranslate nohighlight">\((\mathbf{x}, y)\)</span> pairs, we fit the model to a transformed dataset, consisting of pairs <span class="math notranslate nohighlight">\((\mathbf{z}, y)\)</span>, where the <span class="math notranslate nohighlight">\(i\)</span>th entry of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\mathbf{z}_i = \frac{\mathbf{x}_i - \mu_i}{\sigma_i}.\]</div>
<p>Above, <span class="math notranslate nohighlight">\(\mu_i, \sigma_i\)</span> are the mean and standard deviation of the <span class="math notranslate nohighlight">\(i\)</span>th component of the training set features. After normalization, all features will have values roughly on the interval <span class="math notranslate nohighlight">\([-2, 2]\)</span>. This ensures that each feature makes an equally-weighted contribution to any predictions made.</p>
</section>
</section>
<section id="model-selection">
<h2>Model Selection<a class="headerlink" href="#model-selection" title="Permalink to this heading">#</a></h2>
<p>The task of finding the best model for a supervised learning problem is generaly quite difficult. Unfortunately, there is no tried and true method for determining which model is most appropriate for a specific supervised learning problem. Finding the best model often requires a combination of trial-and-error and expertise in both the dataset and the problem being studied. The good news is that as materials scientists, the kinds of supervised learning problems we encounter in our research often have some underlying physical theory that we can apply directly to the data. Sometimes, by adding some free parameters to these physically informed models, we can obtain a good fit of the data that generalizes well outside the training set. It may also be the case that similar supervised learning problems have been studied before, and a paper or two has been published, along with details of the models used. When it comes to selecting a few models worth trying, a good guiding principle is <a class="reference external" href="https://en.wikipedia.org/wiki/Occam's_razor"><em>Occam’s Razor</em></a>:</p>
<blockquote class="epigraph">
<div><p>“Entities must not be multiplied beyond necessity.”</p>
<p class="attribution">—Occam’s Razor</p>
</div></blockquote>
<p>In the context of machine learning, Occam’s razor is often interpreted as follows: <em>Models with few free parameters and low complexity should be preferred over complex models, unless simple models fail to give good results</em>. When trying to find the best model, start with simple models, and gradually increase the complexity. For each model you want to evaluate, fit it to the training set data and evaluate its accuracy on the validation set. If your model performs poorly, try some more complex models and see if the validation set accuracy increases. As you obtain more experience working with the kinds of models most often used in your field, you will gain more intuition about which models work better than others.</p>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this heading">#</a></h2>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Exercise 1: Training, Validation, and Test Sets<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Let’s get some practice with preparing data. To keep things simple, use the dataset generated by the following code:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">-</span> <span class="mf">0.01</span><span class="o">*</span><span class="n">x_data</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text">To prepare this dataset for use with a model, do the following:</p>
<ol class="arabic simple">
<li><p class="sd-card-text">Plot the data. What kind of model might produce a good fit?</p></li>
<li><p class="sd-card-text">Split the dataset into training, validation and test sets with the standard 80%-20%-20% split. There are a few ways to do this. One way is by shuffling the data using Python’s <a class="reference external" href="https://docs.python.org/3/library/random.html#random.shuffle"><code class="docutils literal notranslate"><span class="pre">random.shuffle</span></code></a> and selecting subsets of the data by their indices. Another (easier) way is to use <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"><code class="docutils literal notranslate"><span class="pre">sklearn.model_selection.train_test_split</span></code></a>. This function can only split the data into two subsets, so you will need to use it twice: once to split off the training set and once more to split the remaining data into the validation and test sets.</p></li>
<li><p class="sd-card-text">Normalize the training, validation, and test sets (transform <span class="math notranslate nohighlight">\(x \rightarrow z\)</span>). You can do this by computing <span class="math notranslate nohighlight">\(\mu_x\)</span> and <span class="math notranslate nohighlight">\(\sigma_x\)</span> with <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.mean.html"><code class="docutils literal notranslate"><span class="pre">np.mean</span></code></a> and <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.std.html"><code class="docutils literal notranslate"><span class="pre">np.std</span></code></a> respectively, or by using <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler"><code class="docutils literal notranslate"><span class="pre">sklearn.preprocessing.StandardScaler</span></code></a>.</p></li>
<li><p class="sd-card-text">Plot the normalized training, validation, and test sets on the same axes. Use a different color for each set.</p></li>
</ol>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Exercise 2: Polynomial Models<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Using the normalized training, validation, and test sets from Exercise 1, generate three different fits using polynomials of degree 1, 2, and 3.</p>
<p class="sd-card-text">Visually inspect the fit of these three models by plotting the validation set and each fit on the same axes. (Plot the non-normalized <span class="math notranslate nohighlight">\((x,y)\)</span> data, not the normalized <span class="math notranslate nohighlight">\((z,y)\)</span> data.)</p>
<p class="sd-card-text">Finally, select the best fit and plot it on the same axes as the test set. When plotting your fit curves, be sure to normalize the inputs to your model before. (In the next section, we will discuss functions that can be used to quantitatively measure goodness of fit. For now, we will measure goodness of fit qualitatively with our eyeballs.)</p>
<hr class="docutils" />
<p class="sd-card-text"><em>Hint:</em> As shown in the example code above, you can fit a polynomial of degree <code class="docutils literal notranslate"><span class="pre">n</span></code> to normalized data using <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html"><code class="docutils literal notranslate"><span class="pre">np.polyfit</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># fit polynomial model to normalized data:</span>
<span class="n">poly_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">poly1d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">z_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">deg</span><span class="o">=</span><span class="n">n</span><span class="p">))</span>

<span class="c1"># evaluate polynomial model at (normalized) point z:</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">poly_model</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details><section id="solutions">
<h3>Solutions:<a class="headerlink" href="#solutions" title="Permalink to this heading">#</a></h3>
<section id="exercise-1-training-validation-and-test-sets">
<h4>Exercise 1: Training, Validation, and Test Sets<a class="headerlink" href="#exercise-1-training-validation-and-test-sets" title="Permalink to this heading">#</a></h4>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># This the dataset we are given:</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">-</span> <span class="mf">0.01</span><span class="o">*</span><span class="n">x_data</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># (1) plot dataset:</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;The data suggests a quadratic (degree 2 polynomial) fit:&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># (2) Split dataset into training, validation, and test sets.</span>
<span class="c1"># First, split data into training and non-training data:</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_nontrain</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_nontrain</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

<span class="c1"># Further split non-training data into validation and test data:</span>
<span class="n">x_validation</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_validation</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x_nontrain</span><span class="p">,</span> <span class="n">y_nontrain</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># (3) Normalize the train/validation/test sets.</span>
<span class="c1"># Estimate mu and sigma for training set:</span>
<span class="n">mu_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
<span class="n">sigma_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>

<span class="c1"># standardize data:</span>
<span class="n">z_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_train</span> <span class="o">-</span> <span class="n">mu_x</span><span class="p">)</span><span class="o">/</span><span class="n">sigma_x</span>
<span class="n">z_validation</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_validation</span> <span class="o">-</span> <span class="n">mu_x</span><span class="p">)</span><span class="o">/</span><span class="n">sigma_x</span>
<span class="n">z_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_test</span> <span class="o">-</span> <span class="n">mu_x</span><span class="p">)</span><span class="o">/</span><span class="n">sigma_x</span>

<span class="c1"># normalize y data:</span>
<span class="n">mu_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">sigma_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>

<span class="c1"># (4) Plot the normalized train/validation/test sets:</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">z_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training Set&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">z_validation</span><span class="p">,</span> <span class="n">y_validation</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation Set&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">z_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Test Set&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;z&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/cfab96ddcf74b9690bc1ae58bd8a0b18ea6040b51aa6ee0b4b060b110b089b44.png" src="../_images/cfab96ddcf74b9690bc1ae58bd8a0b18ea6040b51aa6ee0b4b060b110b089b44.png" />
<img alt="../_images/c662d1ace9b01f5a247b641800ac32db299be0f11c7b766c1d5b2267138d08fb.png" src="../_images/c662d1ace9b01f5a247b641800ac32db299be0f11c7b766c1d5b2267138d08fb.png" />
</div>
</details>
</div>
</section>
<section id="exercise-2-polynomial-models">
<h4>Exercise 2: Polynomial Models<a class="headerlink" href="#exercise-2-polynomial-models" title="Permalink to this heading">#</a></h4>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fit polynomials of degrees 1,2, and 3 to normalized data:</span>
<span class="n">poly_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">poly1d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">z_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">deg</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">poly_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">poly1d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">z_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">deg</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="n">poly_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">poly1d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">z_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">deg</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>

<span class="c1"># generate normalized points for evaluating the model:</span>
<span class="n">x_eval</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x_data</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x_data</span><span class="p">),</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">z_eval</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_eval</span> <span class="o">-</span> <span class="n">mu_x</span><span class="p">)</span><span class="o">/</span><span class="n">sigma_x</span>

<span class="c1"># evaluate models:</span>
<span class="n">yhat_poly_1</span> <span class="o">=</span> <span class="n">poly_1</span><span class="p">(</span><span class="n">z_eval</span><span class="p">)</span>
<span class="n">yhat_poly_2</span> <span class="o">=</span> <span class="n">poly_2</span><span class="p">(</span><span class="n">z_eval</span><span class="p">)</span>
<span class="n">yhat_poly_3</span> <span class="o">=</span> <span class="n">poly_3</span><span class="p">(</span><span class="n">z_eval</span><span class="p">)</span>

<span class="c1"># plot polynomial fits and validation set:</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_validation</span><span class="p">,</span> <span class="n">y_validation</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_eval</span><span class="p">,</span> <span class="n">yhat_poly_1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Degree 1 fit&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_eval</span><span class="p">,</span> <span class="n">yhat_poly_2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Degree 2 fit&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_eval</span><span class="p">,</span> <span class="n">yhat_poly_3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Degree 3 fit&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Note: The second degree and third degree polynomials fit the data</span>
<span class="c1">#       quite well and do not appear to differ much in accuracy. Per</span>
<span class="c1">#       Occam&#39;s razor, the second degree polynomial is the better </span>
<span class="c1">#       model because it has fewer parameters.</span>

<span class="c1"># plot best polynomial fit and test set:</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_eval</span><span class="p">,</span> <span class="n">yhat_poly_2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Degree 2 fit (Best)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ab48804fde41ab0bf7e6b93a8e85bcf199ee47592e3d0a8e15313ae9e53e8b70.png" src="../_images/ab48804fde41ab0bf7e6b93a8e85bcf199ee47592e3d0a8e15313ae9e53e8b70.png" />
<img alt="../_images/0239b586d53c05b5f691e13f3bda80c484179613b52d34a37534509c3b0b2d52.png" src="../_images/0239b586d53c05b5f691e13f3bda80c484179613b52d34a37534509c3b0b2d52.png" />
</div>
</details>
</div>
</section>
</section>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./supervised_learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../ml_intro/math_review.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Mathematics Review</p>
      </div>
    </a>
    <a class="right-next"
       href="fitting_models.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Fitting Supervised Models</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#obtaining-data">Obtaining Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#handling-data">Handling Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-training-set">The Training Set</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-validation-set">The Validation Set</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-test-set">The Test Set</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalizing-data">Normalizing Data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-selection">Model Selection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions">Solutions:</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-training-validation-and-test-sets">Exercise 1: Training, Validation, and Test Sets</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-polynomial-models">Exercise 2: Polynomial Models</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Colin Burdine, E. P. Blair, and Nishat-Tasnim Liza
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>