

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Fitting Supervised Models &#8212; Materials + Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-MBS9YS5YW0"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MBS9YS5YW0');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'supervised_learning/fitting_models';</script>
    <link rel="shortcut icon" href="../_static/logo_small.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Application: Classifying Perovskites" href="basic_models.html" />
    <link rel="prev" title="Supervised Learning" href="supervised_learning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.svg" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.svg" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Materials + ML Workshop
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../intro/intro.html">Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../intro/jupyter.html">Installing Python and Jupyter Notebook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/colab.html">Using Google Colab</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../python/getting_started.html">Getting Started with Python</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../python/python_basics.html">Python Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python/python_logic.html">Logic and Flow Control</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python/loops.html">Loops</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../data_types/python_data_types.html">Python Data Types</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../data_types/sets_and_dicts.html">Sets and Dictionaries</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../functions/functions_classes.html">Python Functions and Classes</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../functions/classes.html">Classes and Object-Oriented Programming</a></li>




</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../scicomp/numpy_scipy.html">Scientific Computing with Numpy and Scipy</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../scicomp/numpy.html">The Numpy Package</a></li>
<li class="toctree-l2"><a class="reference internal" href="../scicomp/scipy.html">The Scipy Package</a></li>



</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../data_vis/data_handling.html">Data Handling and Visualization</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../data_vis/data_vis.html">Data Visualization with Matplotlib</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../matsci/matsci_packages.html">Materials Science Python Packages</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../matsci/ase.html">ASE - The Atomic Simulation Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../matsci/pymatgen_MPRester.html">Pymatgen and the Materials Project API</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ml_intro/ml_intro.html">Introduction to Machine Learning</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ml_intro/statistics_review.html">Statistics Review</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml_intro/math_review.html">Mathematics Review</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="supervised_learning.html">Supervised Learning</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Fitting Supervised Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="basic_models.html">Application: Classifying Perovskites</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../regression/regression.html">Advanced Regression Models</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../regression/kernel_machines.html">Kernel Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../regression/regression_models.html">Application: Bandgap Prediction</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../unsupervised_learning/unsupervised_learning.html">Unsupervised Learning</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../unsupervised_learning/feature_selection.html">Feature Selection and Dimensionality Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unsupervised_learning/clustering.html">Clustering and Distribution Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unsupervised_learning/unsupervised_applications.html">Application: Classifying Superconductors</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../neural_networks/neural_networks.html">Neural Networks</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../neural_networks/training.html">Training Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neural_networks/architectures.html">Types of Neural Network Models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../applications/applications.html">Applications of ML to Materials Science</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/cburdine/materials-ml-workshop" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/cburdine/materials-ml-workshop/issues/new?title=Issue%20on%20page%20%2Fsupervised_learning/fitting_models.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/supervised_learning/fitting_models.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Fitting Supervised Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-functions">Loss Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-models-to-data">Fitting Models to Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient Descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-fitting-a-linear-model">Example: Fitting a Linear Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-loss-functions">Classification Loss Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions">Solutions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-polynomial-regression">Exercise 1: Polynomial Regression</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="fitting-supervised-models">
<h1>Fitting Supervised Models<a class="headerlink" href="#fitting-supervised-models" title="Permalink to this heading">#</a></h1>
<p>In the last section, we introduced the training, validation, and test sets. The training set is used for fitting a supervised model, while the validation and test sets are used for evaluating the model’s accuracy. In this section, we will discuss <em>loss functions</em> and how they are used to evaluate the accuracy of a model. We will also discuss how loss functions can be used to fit models to data.</p>
<section id="loss-functions">
<h2>Loss Functions<a class="headerlink" href="#loss-functions" title="Permalink to this heading">#</a></h2>
<p>A <a class="reference external" href="https://en.wikipedia.org/wiki/Loss_function">loss function</a> measures the error between predictions made by the model (i.e. <span class="math notranslate nohighlight">\(\hat{y}\)</span>) and the true labels <span class="math notranslate nohighlight">\(y\)</span>. Typically, loss functions are a scalar-valued functions that measure the “distance” of predictions from data, meaning the larger the value of the loss function, the worse the accuracy of the model. For individual predictions, the loss function is often denoted as a function <span class="math notranslate nohighlight">\(E: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}\)</span>. For example, if <span class="math notranslate nohighlight">\(y\)</span> is a scalar quantity, the <em>square error</em> loss function on <span class="math notranslate nohighlight">\(y\)</span> is:</p>
<div class="math notranslate nohighlight">
\[E(\hat{y}, y) = (\hat{y} - y)^2\]</div>
<p>For a loss function to be effective, it must satisfy three properties:</p>
<ol class="arabic simple">
<li><p><em>Continuity</em>: <span class="math notranslate nohighlight">\(E(\hat{y},y)\)</span> varies smoothly with <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(\hat{y}\)</span>.</p></li>
<li><p><em>Non-Trivial Differentiability</em>: The derivative of <span class="math notranslate nohighlight">\(E(\hat{y},y)\)</span> with respect to <span class="math notranslate nohighlight">\(\hat{y}\)</span> exists and is not zero everywhere.</p></li>
<li><p><em>Effective Monotonicity</em>: If <span class="math notranslate nohighlight">\(E(\hat{y},y) &lt; E(\hat{y}',y)\)</span> if <span class="math notranslate nohighlight">\(\hat{y}\)</span> is a better prediction of <span class="math notranslate nohighlight">\(y\)</span> than <span class="math notranslate nohighlight">\(\hat{y}\)</span>’.</p></li>
</ol>
<p>Note that property 3 is somewhat subjective, but it is necessary to ensure that the “best” prediction of <span class="math notranslate nohighlight">\(y\)</span> is the one that minimizes <span class="math notranslate nohighlight">\(E(\hat{y}, y)\)</span>. In addition to properties 1-3, we see that the square error loss function satisfies some other useful properties, (i.e. <em>non-negativity</em>: <span class="math notranslate nohighlight">\(E \ge 0\)</span>, <em>symmetry</em>: <span class="math notranslate nohighlight">\(E(\hat{y},y) = E(y,\hat{y})\)</span>, etc.). These properties are desirable, but is not all loss functions will have them. Ultimately, the kind of loss function we choose will depend on what kind of data <span class="math notranslate nohighlight">\(y\)</span> represents and the model we are using.</p>
<p>We now know how to measure the accuracy of a single prediction <span class="math notranslate nohighlight">\(\hat{y} = f(\mathbf{x})\)</span>; however, a model <span class="math notranslate nohighlight">\(f: \mathcal{X} \rightarrow \mathcal{Y}\)</span> may make better better predictions on some data points <span class="math notranslate nohighlight">\((\mathbf{x},y)\)</span> than others. To evaluate a model on a dataset <span class="math notranslate nohighlight">\(\{ (\mathbf{x}_n, y_n) \}_{n=1}^N\)</span> with <span class="math notranslate nohighlight">\(N\)</span> datapoints, we use a <em>model loss function</em>, denoted <span class="math notranslate nohighlight">\(\mathcal{E}(f)\)</span>. Often, <span class="math notranslate nohighlight">\(\mathcal{E}(f)\)</span> is simply the average of a loss function <span class="math notranslate nohighlight">\(E\)</span> evaluated on a dataset:</p>
<div class="math notranslate nohighlight">
\[\mathcal{E}(f) = \frac{1}{N}\sum_{n=1}^N E(f(x),y)\]</div>
<p>These are some commonly used loss functions used when <span class="math notranslate nohighlight">\(y\)</span> is a scalar (or vector) quantity:</p>
<ul>
<li><p>Mean Square Error (MSE):</p>
<div class="math notranslate nohighlight">
\[\mathcal{E}(f) = \frac{1}{N} \sum_{n=1}^N (f(\mathbf{x}_n) - y_n)^2\]</div>
</li>
<li><p>Root Mean Square Error (RMSE):</p>
<div class="math notranslate nohighlight">
\[\mathcal{E}(f) = \sqrt{\frac{1}{N} \sum_{n=1}^N (f(\mathbf{x}_n) - y_n)^2}\]</div>
</li>
<li><p>Mean Absolute Error (MAE):</p>
<div class="math notranslate nohighlight">
\[\mathcal{E}(f) = \frac{1}{N} \sum_{n=1}^N |f(\mathbf{x}_n) - y_n|\]</div>
</li>
<li><p>Maximum Absolute Error:</p>
<div class="math notranslate nohighlight">
\[\mathcal{E}(f) = \max_{n} |f(\mathbf{x}_n) - y_n|\]</div>
</li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The physicists in the room may recognize the RMSE as the <span class="math notranslate nohighlight">\(\ell^2\)</span> norm error (“L2”) and the MAE as the <span class="math notranslate nohighlight">\(\ell^1\)</span> norm error (“L1”). The maximum absolute error is sometimes defined as the <span class="math notranslate nohighlight">\(\ell^\infty\)</span> norm error (“L-infinity”). These names are also commonly used in the machine learning literature.</p>
</div>
</section>
<section id="fitting-models-to-data">
<h2>Fitting Models to Data<a class="headerlink" href="#fitting-models-to-data" title="Permalink to this heading">#</a></h2>
<p>The process of fitting models to data can sometimes be very complex, and the computational method for determining the fit is often specific the type of model being used. These computational methods are called <em>learning algorithms</em>, since they employ iterative procedures for gradually improving the fit of a model.</p>
<p>Since this workshop is not directed at computer scientists, we will not cover these computational methods in detail; however, having at least a cursory understanding of them is crucial in understanding how most models work. As a motivating example, let’s recall a simple 1D polynomial model with degree <span class="math notranslate nohighlight">\(D\)</span>:</p>
<div class="math notranslate nohighlight">
\[f(x) = \sum_{d=0}^D w_d x^d\]</div>
<p>This model has <span class="math notranslate nohighlight">\(N+1\)</span> free parameters (<span class="math notranslate nohighlight">\(w_0, w_2, ..., w_D\)</span>) that must be optimized in order to best fit the data. These free parameters are called the <em>weights</em> of the model. We can also treat <span class="math notranslate nohighlight">\(D\)</span> as a parameter to be optimized; however it can be if <span class="math notranslate nohighlight">\(D\)</span> is too large (i.e. <span class="math notranslate nohighlight">\(D \ge N-1\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the size of the dataset of <span class="math notranslate nohighlight">\((x,y)\)</span> pairs) then there may be multiple best fits where <span class="math notranslate nohighlight">\(\mathcal{E}(f) = 0\)</span>. Parameters like <span class="math notranslate nohighlight">\(D\)</span> that influence the number of weights in the model are called <em>hyperparameters</em>. These values are often held constant during the fitting process.</p>
<p>Suppose that we use the MSE loss function to determine the best fit of <span class="math notranslate nohighlight">\(f(x)\)</span> to a dataset <span class="math notranslate nohighlight">\(\{ (x_n,y_n) \}_{n=1}^N\)</span> consisting of <span class="math notranslate nohighlight">\(N\)</span> <span class="math notranslate nohighlight">\(x\)</span>-<span class="math notranslate nohighlight">\(y\)</span> pairs. Then, the model loss can be written in terms of the model weights <span class="math notranslate nohighlight">\(w_0, w_1, ..., w_D\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathcal{E}(f) = \frac{1}{N} \sum_{n=1}^N \left( \left[\sum_{d=0}^D w_d x_n^d\right] - y_n \right)^2\]</div>
<p>We find the best fit of <span class="math notranslate nohighlight">\(f\)</span> to the dataset by minimizing <span class="math notranslate nohighlight">\(\mathcal{E}(f)\)</span> with respect to the weights. At first, this task may seem quite daunting, especially since there are infinitely many weight configurations to try. Even if we tried one of <span class="math notranslate nohighlight">\(n\)</span> different configurations per weight, we would still need to search a space of <span class="math notranslate nohighlight">\(D^n\)</span> different weight configurations. Surely there must be a better way of searching this exponentially large space for the optimal weights. This is where <em>learning algorithms</em> come into play.</p>
<p>A learning algorithm is a computational procedure for gradually improving a model fit from some initial weight configuration to a weight configuration that produces a lower <span class="math notranslate nohighlight">\(\mathcal{E}(f)\)</span>. By repeating this process over and over, <span class="math notranslate nohighlight">\(\mathcal{E}(f)\)</span> will eventually converge to a local minimum. For now, we will discuss the most commonly used learning algorithm: <em>gradient descent</em>.</p>
</section>
<section id="gradient-descent">
<h2>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this heading">#</a></h2>
<p>As its name suggests, <em>gradient descent</em> is a learning algorithm that minimizes <span class="math notranslate nohighlight">\(\mathcal{E}(f)\)</span> by descending the gradient of the loss function <span class="math notranslate nohighlight">\(\mathcal{E}\)</span> with respect to its weights. To denote the gradient of a model  with respect to its weights, we use the shorthand:</p>
<div class="math notranslate nohighlight">
\[\nabla_{w} f(\mathbf{x}) = \begin{bmatrix} \dfrac{\partial f}{\partial w_0}(\mathbf{x}) &amp; \dfrac{\partial f}{\partial w_1}(\mathbf{x}) &amp; \dots &amp; \dfrac{\partial f}{\partial w_D}(\mathbf{x}) \end{bmatrix}^T\]</div>
<p>Any loss function <span class="math notranslate nohighlight">\(E(\hat{y},y)\)</span> that satisfies properties 1-3 is continuous and differentiable, meaning <span class="math notranslate nohighlight">\(\frac{\partial E}{\partial \hat{y}}\)</span> exists. This means that we can compute the gradient of a loss function <span class="math notranslate nohighlight">\(E\)</span> with respect to the weights for a single data point <span class="math notranslate nohighlight">\((\mathbf{x}, y)\)</span> using chain rule:</p>
<div class="math notranslate nohighlight">
\[\nabla_w E(f(\mathbf{x}), y) = \dfrac{\partial E}{\partial \hat{y}}(f(\mathbf{x}), y) \cdot \nabla_w f(\mathbf{x})\]</div>
<p>Note that the first quantity in the product above is a scalar, and the second quantity is a vector, meaning the result is a vector. We can interpret <span class="math notranslate nohighlight">\(\nabla_w E\)</span> as a vector of weight “adjustments” that points in the direction of greatest increase in <span class="math notranslate nohighlight">\(\mathcal{E}\)</span> with respect to to the weights and the data point <span class="math notranslate nohighlight">\((\mathbf{x},y)\)</span>. The magnitude of <span class="math notranslate nohighlight">\(\nabla_w E\)</span> can be interpreted as the “steepness” of the surface of <span class="math notranslate nohighlight">\(E\)</span> in the direction of greatest increase.</p>
<p>If the model loss function <span class="math notranslate nohighlight">\(\mathcal{E}(f)\)</span> is an average of the single datapoint loss <span class="math notranslate nohighlight">\(E(f(\mathbf{x}),y)\)</span> over a dataset <span class="math notranslate nohighlight">\(\{ (\mathbf{x}_n,y_n) \}_{n=1}^N\)</span>, the <a class="reference external" href="https://en.wikipedia.org/wiki/Linear_map">linearity</a> of the gradient operator implies that:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned} \nabla_w \mathcal{E}(f) &amp;= \nabla_w \left( \frac{1}{N} \sum_{n=1}^N E(f(\mathbf{x}_n), y_n)\right) = \frac{1}{N} \sum_{n=1}^N \nabla_{w} E(f(\mathbf{x}_n),y_n) \\ &amp;= \frac{1}{N} \sum_{n=1}^N \dfrac{\partial E}{\partial \hat{y}}(f(\mathbf{x}_n), y_n) \cdot \nabla_w f(\mathbf{x}_n) \end{aligned}\end{split}\]</div>
<p>The vector <span class="math notranslate nohighlight">\(\nabla_w \mathcal{E}(f)\)</span> points in the direction of greatest increase of the model loss <span class="math notranslate nohighlight">\(\mathcal{E}(f)\)</span>. Since our goal is to minimize <span class="math notranslate nohighlight">\(\mathcal{E}(f)\)</span>, we should adjust the weights in the direction of <span class="math notranslate nohighlight">\(-\nabla_w \mathcal{E}(f)\)</span>, thereby <em>descending</em> the gradient. We can think of gradient descent this as a time-stepped procedure, where the gradient with respect to the weight vector <span class="math notranslate nohighlight">\(\mathbf{w} = \begin{bmatrix} w_0 &amp; w_1 &amp; ... &amp; w_D \end{bmatrix}^T\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> is computed and used to update the weights for time <span class="math notranslate nohighlight">\(t+1\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \nabla_w \mathcal{E}(f)\]</div>
<p>This process is illustrated for a simple 2D vector of weights <span class="math notranslate nohighlight">\(\mathbf{w} = \begin{bmatrix} w_0 &amp; w_1 \end{bmatrix}\)</span> below:</p>
<p><img alt="Illustration of Gradient Descent in 2D" src="../_images/grad_descent.svg" /></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Here’s another one for the physicists in the room: If we interpret <span class="math notranslate nohighlight">\(\mathcal{E}\)</span> as a potential energy function within the <span class="math notranslate nohighlight">\(D\)</span>+1-dimensional space of weight configurations, we can interpret <span class="math notranslate nohighlight">\(-\nabla_w \mathcal{E}(f)\)</span> as proportional to the net “force” vector exerted on the current weight configuration by each datapoint. When the model fit converges to a local minimum of <span class="math notranslate nohighlight">\(\mathcal{E}\)</span>, the system reaches a stable equilibrium where the net “force” on the weight configuration is zero. This is somewhat analogous to the dynamics of a ball rolling down a hill:</p>
<p><img alt="Gradient Descent Balls" src="../_images/grad_local.svg" /></p>
</div>
<p>Ideally, we should expect <span class="math notranslate nohighlight">\(\mathcal{E}(f)\)</span> to gradually decrease with each timestep until <span class="math notranslate nohighlight">\(-\nabla_w \mathcal{E}(f) \approx \mathbf{0}\)</span>, at which point <span class="math notranslate nohighlight">\(\mathcal{E}(f)\)</span> has attained a local minimum. Take note that at some points where <span class="math notranslate nohighlight">\(\mathcal{E}(f)\)</span> is steep, the magnitude of the gradient, <span class="math notranslate nohighlight">\(\lVert -\nabla_w \mathcal{E}(f) \rVert\)</span>, can be quite large. This can sometimes cause the gradient descent procedure to completely overstep local minima and even increase <span class="math notranslate nohighlight">\(\mathcal{E}(f)\)</span> from last step. Problems can also be encountered when <span class="math notranslate nohighlight">\(\lVert -\nabla_w \mathcal{E}(f) \rVert\)</span> is very small, as many small steps may be required to reach the nearest minimum. To avoid these pathologies, we can set the changes in the weight vector <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> to have a fixed magnitude <span class="math notranslate nohighlight">\(\eta\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} + \eta \frac{-\nabla_w \mathcal{E}(f)}{\lVert{-\nabla_w \mathcal{E}(f)}\rVert}\]</div>
<p>The value of <span class="math notranslate nohighlight">\(\eta\)</span> is called the <em>learning rate</em>. Note that if <span class="math notranslate nohighlight">\(\eta\)</span> is set too high, then the model may overstep local minima, but if <span class="math notranslate nohighlight">\(\eta\)</span> is set too low, then the gradient descent may take a long time to converge. Choosing the right value of <span class="math notranslate nohighlight">\(\eta\)</span> is important for obtaining a good fit of the data.</p>
</section>
<section id="example-fitting-a-linear-model">
<h2>Example: Fitting a Linear Model<a class="headerlink" href="#example-fitting-a-linear-model" title="Permalink to this heading">#</a></h2>
<p>Let’s write some Python code that uses gradient descent to fit a 1D linear model (<span class="math notranslate nohighlight">\(f(x) = w_0 + w_1x\)</span>) to data with the MSE model loss function.</p>
<p>Let’s start by writing the functions necessary for gradient descent, namely <span class="math notranslate nohighlight">\(E(\hat{y}, y), \partial E/ \partial \)</span>\hat{y}<span class="math notranslate nohighlight">\(, f(x)\)</span>, and <span class="math notranslate nohighlight">\(\nabla_w f(x)\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; computes the square error loss &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span> <span class="nf">loss_yhat_deriv</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; computes loss dE/d yhat &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">linear_f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; computes linear model: yhat = w0 + w1*x &quot;&quot;&quot;</span>
    <span class="n">yhat</span> <span class="o">=</span>  <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span>
    <span class="k">return</span> <span class="n">yhat</span>

<span class="k">def</span> <span class="nf">linear_f_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; computes gradient of linear model: yhat = w0 + w1*x &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span> <span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we write the gradient descent algorithm:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span> <span class="n">data_y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> 
                     <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.002</span><span class="p">,</span> 
                     <span class="n">tolerance</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">,</span> 
                     <span class="n">max_steps</span><span class="o">=</span><span class="mi">10</span><span class="o">**</span><span class="mi">4</span><span class="p">):</span>

    <span class="c1"># compute initial loss:</span>
    <span class="n">initial_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">linear_f</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span> <span class="n">data_y</span><span class="p">))</span>
    <span class="n">loss_history</span> <span class="o">=</span> <span class="p">[</span> <span class="n">initial_loss</span> <span class="p">]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>

        <span class="c1"># compute gradient of model loss:</span>
        <span class="n">grad_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
            <span class="n">loss_yhat_deriv</span><span class="p">(</span><span class="n">linear_f</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span><span class="n">w</span><span class="p">),</span><span class="n">data_y</span><span class="p">)</span> <span class="o">*</span> <span class="n">linear_f_grad</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span><span class="n">w</span><span class="p">),</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># update weights:</span>
        <span class="n">w</span> <span class="o">+=</span> <span class="o">-</span><span class="n">learning_rate</span><span class="o">*</span><span class="n">grad_loss</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="o">-</span><span class="n">grad_loss</span><span class="p">)</span>
        
        <span class="c1"># record loss after update:</span>
        <span class="n">step_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">linear_f</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span> <span class="n">data_y</span><span class="p">))</span>
        <span class="n">loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">step_loss</span><span class="p">)</span>
        
        <span class="c1"># stop if the decrease in loss is small:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">loss_history</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">loss_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">tolerance</span><span class="p">:</span>
            <span class="k">break</span>

    <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">loss_history</span>
</pre></div>
</div>
</div>
</div>
<p>To see how well our algorithm works, let’s see how well the model fits some 1D linear data. We will generate this data such that is is already approximately normalized  (<span class="math notranslate nohighlight">\(\mu_x \approx 0, \sigma_x \approx 1\)</span>). Using the following code, we compare the weights learned though gradient descent with the theoretically optimal weights:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate data with target weights:</span>
<span class="n">w_target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.1</span> <span class="p">])</span>
<span class="n">data_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">data_y</span> <span class="o">=</span> <span class="n">w_target</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">w_target</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">data_x</span>

<span class="c1"># initialize weights to 0:</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>

<span class="c1"># do gradient descent:</span>
<span class="n">w</span><span class="p">,</span> <span class="n">loss_history</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span> <span class="n">data_y</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

<span class="c1"># compare fitted weights with target</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Optimal Weights:&#39;</span><span class="p">,</span> <span class="n">w_target</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Gradient Descent Weights:&#39;</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimal Weights: [2.5 0.1]
Gradient Descent Weights: [2.49977044 0.10000066]
</pre></div>
</div>
</div>
</div>
<p>We observe that the weights are quite close to the optimal values. We can verify that <span class="math notranslate nohighlight">\(\mathcal{E}(f)\)</span> was minimized by plotting the <code class="docutils literal notranslate"><span class="pre">loss_history</span></code> returned by the gradient descent algorithm:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>


<span class="c1"># plot loss history:</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mf">1.5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_history</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Step $t$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$\mathcal</span><span class="si">{E}</span><span class="s1">(f)$ (MSE)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/93ab8e0b01ec06cf657996a2b78300b0af20b417cbf27328b6d2131e69efd1f2.png" src="../_images/93ab8e0b01ec06cf657996a2b78300b0af20b417cbf27328b6d2131e69efd1f2.png" />
</div>
</div>
<p>Plotting the data and the estimated fit, we see that the fit is quite reasonable:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.1</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">y_pts</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x_pts</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span> <span class="n">data_y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Dataset&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pts</span><span class="p">,</span> <span class="n">y_pts</span><span class="p">,</span> <span class="s1">&#39;g:&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Linear model fit&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/3943cc89c30d764cbf380a4e27b8338f9e581dc84da97f618d9e642728959221.png" src="../_images/3943cc89c30d764cbf380a4e27b8338f9e581dc84da97f618d9e642728959221.png" />
</div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>In this exercise, we did not apply normalization to the <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> data, since the data was approximately normalized to begin with. Generally, it is a good idea to normalize data (transform <span class="math notranslate nohighlight">\(\mathbf{x} \rightarrow \mathbf{z}\)</span>) and then apply gradient descent to the normalized data. This ensures that the model is not too sensitive to any particular feature, resulting in a better fit.</p>
</div>
</section>
<section id="classification-loss-functions">
<h2>Classification Loss Functions<a class="headerlink" href="#classification-loss-functions" title="Permalink to this heading">#</a></h2>
<p>So far, we have only discussed loss functions used for when <span class="math notranslate nohighlight">\(y\)</span> is a scalar quantity. This is most often the case for regression problems, but what about classification problems, where <span class="math notranslate nohighlight">\(\hat{y}\)</span> can only be one of a finite number of classes? An intuitive loss function for classes is the so-called <a class="reference external" href="https://en.wikipedia.org/wiki/Kronecker_delta"><em>Kronecker delta function</em></a>, given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split} E(\hat{y}, y) = \delta(\hat{y},y) = \begin{cases} 
1 &amp; \hat{y} = y \\ 
0 &amp; \hat{y} \neq y
\end{cases}\end{split}\]</div>
<p>The simplest model loss function for classification tasks is the <em>classification accuracy</em>:</p>
<div class="math notranslate nohighlight">
\[\mathcal{E}(f) = \frac{1}{N} \sum_{n=1}^N \delta(\hat{y},y) = \frac{\text{# Correct}}{Total}\]</div>
<p>While the classification accuracy may be useful for evaluating a classification model, we see that is uses the Kronecker delta loss, which only satisfies property 3 of effective loss functions. Due to the discrete nature of classes, fitting classification models using gradient descent may prove challenging unless we can make the loss function continuous.</p>
<p>We can make the loss continuous by by re-formulating the classifier model <span class="math notranslate nohighlight">\(f(x)\)</span> as one that predicts the probability of <span class="math notranslate nohighlight">\(f(x)\)</span> being a member of each class instead of the class directly. This re-frames the problem as <em>logistic regression</em>, which is a type of regression that deals with the prediction of probability distributions. For now, we will only consider the problem of binary (i.e. two-class) classification, where <span class="math notranslate nohighlight">\(f(x)\)</span> only needs to predict the probability of the first class.</p>
<p>For a binary logistic regression model <span class="math notranslate nohighlight">\(f: \mathcal{X} \rightarrow [0,1]\)</span>, we could use any of the loss functions mentioned above to fit <span class="math notranslate nohighlight">\(f\)</span> to data points <span class="math notranslate nohighlight">\((\mathbf{x},y)\)</span> where <span class="math notranslate nohighlight">\(y = 0\)</span> for the first class and <span class="math notranslate nohighlight">\(y = 1\)</span> for the second class. However, it has been shown that logistic regression models trained by gradient descent can converge faster with the <a class="reference external" href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression"><em>binary crossentropy loss</em></a> (sometimes also called the <em>log loss</em>):</p>
<div class="math notranslate nohighlight">
\[E(\hat{y}, y) = y\log(\hat{y}) + (1-y)\log(1-\hat{y}),\qquad \mathcal{E}(f) = \frac{1}{N}\sum_{n=1}^N \left[ y\log(\hat{y}) + (1-y)\log(1-\hat{y}) \right]\]</div>
<p>This loss function can be used to fit the binary logistic regression model to data. Note that in order to convert the probability estimates <span class="math notranslate nohighlight">\(\hat{y}\)</span> to discrete classifications, we simply select the class based on whether <span class="math notranslate nohighlight">\(p &gt; 0.5\)</span> or <span class="math notranslate nohighlight">\(p &lt; 0.5\)</span>.</p>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this heading">#</a></h2>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Exercise 1: Polynomial Regression<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Let’s kick things up a notch and generalize our gradient descent algorithm to fit arbitrary degree polynomial models using the MSE loss function. This might seem difficult at first, but upon inspecting the <code class="docutils literal notranslate"><span class="pre">gradient_descent</span></code> function, we see that the only thing that we need to change is the <code class="docutils literal notranslate"><span class="pre">linear_f_grad</span></code> and <code class="docutils literal notranslate"><span class="pre">linear_f</span></code> function.</p>
<p class="sd-card-text">First, make a copy of the <code class="docutils literal notranslate"><span class="pre">gradient_descent</span></code> function and call it <code class="docutils literal notranslate"><span class="pre">poly_gradient_descent</span></code>, changing only the following lines:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    # compute gradient of model loss:
    grad_loss = np.mean(
        loss_yhat_deriv(poly_f(data_x,w),data_y) * poly_f_grad(data_x,w),
        axis=1)
    
    ...
    
    # record loss after update:
        step_loss = np.mean(loss(poly_f(data_x, w), data_y))
        loss_history.append(step_loss)`
</pre></div>
</div>
<p class="sd-card-text">Above the new <code class="docutils literal notranslate"><span class="pre">poly_gradient_descent</span></code> function, add the following new functions, which are called within <code class="docutils literal notranslate"><span class="pre">poly_gradient_descent</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">poly_f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; computes a polynomial model: yhat = w0 + w1*x + w2*x^2 + ... &quot;&quot;&quot;</span>
    <span class="n">yhat</span> <span class="o">=</span>  <span class="s1">&#39;&lt;your code here&gt;&#39;</span>
    <span class="k">return</span> <span class="n">yhat</span>

<span class="k">def</span> <span class="nf">poly_f_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; computes gradient of a polynomial model &quot;&quot;&quot;</span>
    <span class="n">grad_f</span> <span class="o">=</span> <span class="s1">&#39;&lt;your code here&gt;&#39;</span>
    <span class="k">return</span> <span class="n">grad_f</span>
</pre></div>
</div>
<p class="sd-card-text">In <code class="docutils literal notranslate"><span class="pre">poly_f(x,w)</span></code> and <code class="docutils literal notranslate"><span class="pre">poly_f_grad(x,w)</span></code>, you can assume that <code class="docutils literal notranslate"><span class="pre">w</span></code> is a 1D numpy array of length <span class="math notranslate nohighlight">\(D+1\)</span>, (<span class="math notranslate nohighlight">\(D\)</span> is the degree of the polynomial) with weights in ascending order: <span class="math notranslate nohighlight">\([ w_0, w_1, ...., w_D ]\)</span>. Your task is to determine what should go in the place of <code class="docutils literal notranslate"><span class="pre">'&lt;your</span> <span class="pre">code</span> <span class="pre">here&gt;'</span></code>.</p>
<p class="sd-card-text">Using your new polynomial gradient descent function, determine the weights of a degree 6 polynomial that fits the following data:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">data_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">data_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x_data</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text">Plot both the data and your fitted degree-6 polynomial.
Don’t worry about normalization or splitting into train/validation/test sets here.</p>
</div>
</details><section id="solutions">
<h3>Solutions<a class="headerlink" href="#solutions" title="Permalink to this heading">#</a></h3>
<section id="exercise-1-polynomial-regression">
<h4>Exercise 1: Polynomial Regression<a class="headerlink" href="#exercise-1-polynomial-regression" title="Permalink to this heading">#</a></h4>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">poly_f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; computes a polynomial model: yhat = w0 + w1*x + w2*x^2 + ... &quot;&quot;&quot;</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span> <span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="n">i</span>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">))</span> <span class="p">)</span>
    <span class="k">return</span> <span class="n">yhat</span>

<span class="k">def</span> <span class="nf">poly_f_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; computes gradient of a polynomial model &quot;&quot;&quot;</span>
    <span class="n">grad_f</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="n">x</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">))</span> <span class="p">])</span>
    <span class="k">return</span> <span class="n">grad_f</span>

<span class="k">def</span> <span class="nf">poly_gradient_descent</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span> <span class="n">data_y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> 
                     <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.002</span><span class="p">,</span> 
                     <span class="n">tolerance</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> 
                     <span class="n">max_steps</span><span class="o">=</span><span class="mi">10</span><span class="o">**</span><span class="mi">4</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Fits a n-degree polynomial model using gradient descent. &quot;&quot;&quot;</span>

    <span class="c1"># compute initial loss:</span>
    <span class="n">initial_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">linear_f</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span> <span class="n">data_y</span><span class="p">))</span>
    <span class="n">loss_history</span> <span class="o">=</span> <span class="p">[</span> <span class="n">initial_loss</span> <span class="p">]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>

        <span class="c1"># compute gradient of model loss:</span>
        <span class="n">grad_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
            <span class="n">loss_yhat_deriv</span><span class="p">(</span><span class="n">poly_f</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span><span class="n">w</span><span class="p">),</span><span class="n">data_y</span><span class="p">)</span> <span class="o">*</span> <span class="n">poly_f_grad</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span><span class="n">w</span><span class="p">),</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># update weights:</span>
        <span class="n">w</span> <span class="o">+=</span> <span class="o">-</span><span class="n">learning_rate</span><span class="o">*</span><span class="n">grad_loss</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="o">-</span><span class="n">grad_loss</span><span class="p">)</span>
        
        <span class="c1"># record loss after update:</span>
        <span class="n">step_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">poly_f</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span> <span class="n">data_y</span><span class="p">))</span>
        <span class="n">loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">step_loss</span><span class="p">)</span>
        
        <span class="c1"># stop if the decrease in loss is small:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">loss_history</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">loss_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">tolerance</span><span class="p">:</span>
            <span class="k">break</span>

    <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">loss_history</span>

<span class="c1"># initialize the dataset:</span>
<span class="n">data_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">data_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">data_x</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>

<span class="c1"># initialize weights of a polynomial model:</span>
<span class="n">poly_degree</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">poly_degree</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># fit polynomial with gradient descent:</span>
<span class="n">w</span><span class="p">,</span> <span class="n">history</span> <span class="o">=</span> <span class="n">poly_gradient_descent</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span> <span class="n">data_y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">max_steps</span><span class="o">=</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># plot data and polynomial fit:</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span> <span class="n">data_y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Dataset&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span> <span class="n">poly_f</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Polynomial fit&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1f05a7bf058e5fc194fbee6573db01e285ffdc19616898a9579f560a64371b05.png" src="../_images/1f05a7bf058e5fc194fbee6573db01e285ffdc19616898a9579f560a64371b05.png" />
</div>
</details>
</div>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./supervised_learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="supervised_learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Supervised Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="basic_models.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Application: Classifying Perovskites</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-functions">Loss Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-models-to-data">Fitting Models to Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient Descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-fitting-a-linear-model">Example: Fitting a Linear Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-loss-functions">Classification Loss Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions">Solutions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-polynomial-regression">Exercise 1: Polynomial Regression</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Colin Burdine, E. P. Blair, and Nishat-Tasnim Liza
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>