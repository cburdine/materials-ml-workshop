
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Statistics Review &#8212; Materials + Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-MBS9YS5YW0"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MBS9YS5YW0');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MBS9YS5YW0');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ml_intro/statistics_review';</script>
    <link rel="icon" href="../_static/logo_small.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Mathematics Review" href="math_review.html" />
    <link rel="prev" title="Introduction to Machine Learning" href="ml_intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.svg" class="logo__image only-light" alt="Materials + Machine Learning - Home"/>
    <script>document.write(`<img src="../_static/logo.svg" class="logo__image only-dark" alt="Materials + Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Materials + ML Workshop
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../intro/intro.html">Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../intro/jupyter.html">Installing Python and Jupyter Lab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/colab.html">Using Google Colab</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../python/getting_started.html">Getting Started with Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../python/python_basics.html">Python Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python/python_logic.html">Logic and Flow Control</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python/loops.html">Loops</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../data_types/python_data_types.html">Python Data Types</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../data_types/sets_and_dicts.html">Sets and Dictionaries</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../functions/functions_classes.html">Python Functions and Classes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../functions/classes.html">Classes and Object-Oriented Programming</a></li>




</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../scicomp/numpy_scipy.html">Scientific Computing with Numpy and Scipy</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../scicomp/numpy.html">The Numpy Package</a></li>
<li class="toctree-l2"><a class="reference internal" href="../scicomp/scipy.html">The Scipy Package</a></li>



</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../data_vis/data_handling.html">Data Analysis and Visualization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../data_vis/data_vis.html">Data Visualization with Matplotlib</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../matsci/matsci_packages.html">Materials Science Python Packages</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../matsci/ase.html">ASE - The Atomic Simulation Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../matsci/pymatgen_MPRester.html">Pymatgen and the Materials Project API</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ml_intro.html">Introduction to Machine Learning</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Statistics Review</a></li>
<li class="toctree-l2"><a class="reference internal" href="math_review.html">Mathematics Review</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/cburdine/materials-ml-workshop" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/cburdine/materials-ml-workshop/issues/new?title=Issue%20on%20page%20%2Fml_intro/statistics_review.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/ml_intro/statistics_review.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Statistics Review</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-distributions">Probability Distributions:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-binomial-distribution">The Binomial Distribution:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-normal-distribution">The Normal Distribution:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-central-limit-theorem">The Central Limit Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hypothesis-testing">Hypothesis Testing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-conductor-vs-insulator-classifier">Example: Conductor vs. Insulator Classifier</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-multivariate-normal-distribution">The Multivariate Normal Distribution:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions">Solutions:</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-comparing-two-classifiers">Exercise 1: Comparing Two Classifiers</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-fitting-a-multivariate-normal-distribution">Exercise 2: Fitting a Multivariate Normal Distribution:</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="statistics-review">
<h1>Statistics Review<a class="headerlink" href="#statistics-review" title="Link to this heading">#</a></h1>
<p>Before we dive into Machine Learning, we will do a brief review of the following concepts from statistics:</p>
<ul class="simple">
<li><p>Probability Distributions</p></li>
<li><p>The Binomial Distribution</p></li>
<li><p>The Normal Distribution</p></li>
<li><p>The Central Limit Theorem</p></li>
<li><p>Hypothesis Testing</p></li>
<li><p>The Multivariate Normal Distribution</p></li>
</ul>
<p>If you are already familiar with these concepts, feel free to skip this section or to only read the sections you need to review.</p>
<section id="probability-distributions">
<h2>Probability Distributions:<a class="headerlink" href="#probability-distributions" title="Link to this heading">#</a></h2>
<p>A <em>random variable</em> <span class="math notranslate nohighlight">\(X\)</span> is a variable that can take on one of a number of possible values with associated probabilities. The set of possible values attainable by <span class="math notranslate nohighlight">\(X\)</span> is called the <em>support</em> of <span class="math notranslate nohighlight">\(X\)</span>. In this workshop, we will use the notation <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> to denote the support of a random variable <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Random variables are often defined as <em>probability distributions</em> over their respective supports. A probability distribution is a function that assigns a likelihood to each possible value <span class="math notranslate nohighlight">\(x\)</span> in the support <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. Probability distributions can be <em>discrete</em> (i.e. when <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> is countable) or <em>continuous</em> (when the <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> is not countable). For example, the probability distribution of outcomes for rolling a six-sided dice is discrete, whereas the distribution of darts thrown at a dartboard is continuous. In this workshop, we will use the notation <span class="math notranslate nohighlight">\(p(x)\)</span> to denote probability distributions.</p>
<p>In order for a probability distribution to be well-defined, we require the distribution to be <em>normalized</em>, meaning all probabilities add up to 1. This means that:</p>
<div class="math notranslate nohighlight">
\[\begin{split}1 = \begin{cases} \sum_{x} p(x) &amp;  [\text{for discrete } p(x)]\\ \int_\mathcal{X} p(x)\ dx &amp; [\text{for continuous } p(x)] \end{cases}\end{split}\]</div>
<p>The <em>expected value</em> of a distribution <span class="math notranslate nohighlight">\(p(x)\)</span>, denoted <span class="math notranslate nohighlight">\(\mathbb{E}[x]\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbb{E}[p(x)] = \begin{cases} \sum_{x} p(x)x &amp;   [\text{for discrete } p(x)]\quad \\ \int_{\mathcal{X}} p(x)x\ dx  &amp; [\text{for continuous } p(x)] \end{cases}\end{split}\]</div>
<p>The <em>expected value</em> of a random variable, sometimes called the <em>average</em> value or <em>mean</em> value, is the average of all possible outcomes weighted according to their likelihoods. The mean of a random variable is also often denoted by <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p>The <em>variance</em> of a random variable <span class="math notranslate nohighlight">\(X\)</span> describes the degree to which the distribution deviates from the mean <span class="math notranslate nohighlight">\(\mu\)</span>. It is often denoted by <span class="math notranslate nohighlight">\(\sigma^2\)</span>, and is given by:</p>
<div class="math notranslate nohighlight">
\[\sigma^2 = \mathbb{E}[ (X - \mu)^2 ] = \sum_{x} (x - \mu)^2 = \int_\mathcal{X} (x - \mu)^2\ dx\]</div>
<p>The variance can also be computed by the equivalent formula:</p>
<div class="math notranslate nohighlight">
\[\sigma^2 = \mathbb{E}[X^2] - \mathbb{E}[X]^2\]</div>
<p>The <em>standard deviation</em> of a distribution, denoted by <span class="math notranslate nohighlight">\(\sigma\)</span>, is the square root of the variance <span class="math notranslate nohighlight">\(\sigma\)</span>. Roughly speaking, <span class="math notranslate nohighlight">\(\sigma\)</span> measures how far we expect a random variable to deviate from its mean. As a general rule of thumb, if an outcome is more than <span class="math notranslate nohighlight">\(2\sigma\)</span> away from <span class="math notranslate nohighlight">\(\mu\)</span>, it is considered to be a statistically significant deviation.</p>
</section>
<section id="the-binomial-distribution">
<h2>The Binomial Distribution:<a class="headerlink" href="#the-binomial-distribution" title="Link to this heading">#</a></h2>
<p>The <em>binomial distribution</em> is a discrete probability distribution that models the number of successes in a set of <span class="math notranslate nohighlight">\(N\)</span> independent trials, where each trial succeeds with a fixed probability <span class="math notranslate nohighlight">\(p\)</span>. A random variable <span class="math notranslate nohighlight">\(X\)</span> that is binomially distributed has support <span class="math notranslate nohighlight">\(\mathcal{X} = \{ 0, 1, ..., N \}\)</span> and probability distribution:</p>
<div class="math notranslate nohighlight">
\[p(x) = p^{x} (1-p)^{N-x} \binom{N}{x} = p^x (1-p)^{N-x} \left[ \frac{N!}{x!(N-x)!} \right]\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We emphasize that <span class="math notranslate nohighlight">\(p(x)\)</span> is not the same as <span class="math notranslate nohighlight">\(p\)</span>. <span class="math notranslate nohighlight">\(p\)</span> is the probability of success within any single, independent trial (experiment), so that <span class="math notranslate nohighlight">\((1-p)\)</span> is the probability of failure in any trial. We interpret <span class="math notranslate nohighlight">\(p(x)\)</span> as the probability that in a set of <span class="math notranslate nohighlight">\(N\)</span> trials, exactly <span class="math notranslate nohighlight">\(x\)</span> trials are successful, and <span class="math notranslate nohighlight">\(N-x\)</span> trials are failures.</p>
</div>
<p>Let’s write some Python code to visualize a Binomial distribution. We can compute the probability distribution by hand, or we can use the <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html"><code class="docutils literal notranslate"><span class="pre">scipy.stats.binom.pmf</span></code></a> function:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">binom</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># number of trials</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.3</span> <span class="c1"># probability of trial success</span>

<span class="c1"># evaluate probability distribution:</span>
<span class="n">x_support</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x_probs</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x_support</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>

<span class="c1"># plot distribution:</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">x_support</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x_support</span><span class="p">,</span> <span class="n">x_probs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;p(x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/82589561d11163ea19521a022143c315e3b235bdb5d9cc7ff1300a1d5f02c633.png" src="../_images/82589561d11163ea19521a022143c315e3b235bdb5d9cc7ff1300a1d5f02c633.png" />
</div>
</div>
<p>The mean and variance of this distribution are <span class="math notranslate nohighlight">\(\mu = Np\)</span> and <span class="math notranslate nohighlight">\(\sigma^2 = np(1-p)\)</span> respectively.</p>
</section>
<section id="the-normal-distribution">
<h2>The Normal Distribution:<a class="headerlink" href="#the-normal-distribution" title="Link to this heading">#</a></h2>
<p>The <em>normal distribution</em> (also called the <em>Gaussian distribution</em>) is perhaps the most important continuous distribution in statistics. This distribution is parameterized by its mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> and has support <span class="math notranslate nohighlight">\(\mathcal{X} = (-\infty, \infty)\)</span>. The distribution is:</p>
<div class="math notranslate nohighlight">
\[p(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{1}{2}\left[\frac{x - \mu}{\sigma}\right]^2\right)\]</div>
<p>If we plot this distribution (using <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html#scipy-stats-norm"><code class="docutils literal notranslate"><span class="pre">scipy.stats.norm.pdf</span></code></a>), we obtain the familiar “bell curve” shape:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">norm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">mu</span> <span class="o">=</span> <span class="mf">0.0</span> <span class="c1"># mean of distribution</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="c1"># standard deviation of distribution</span>

<span class="c1"># evaluate probability distribution:</span>
<span class="n">x_pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">mu</span><span class="o">-</span><span class="mi">3</span><span class="o">*</span><span class="n">sigma</span><span class="p">,</span> <span class="n">mu</span><span class="o">+</span><span class="mi">3</span><span class="o">*</span><span class="n">sigma</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">x_probs</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_pts</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>

<span class="c1"># plot distribution:</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_pts</span><span class="p">,</span> <span class="n">x_probs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;p(y)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/59b2862cbaddd6c95870dd4d0cbf13a7eb78970bada55357669aa246f539b5eb.png" src="../_images/59b2862cbaddd6c95870dd4d0cbf13a7eb78970bada55357669aa246f539b5eb.png" />
</div>
</div>
</section>
<section id="the-central-limit-theorem">
<h2>The Central Limit Theorem<a class="headerlink" href="#the-central-limit-theorem" title="Link to this heading">#</a></h2>
<p>The <em><a class="reference external" href="https://en.wikipedia.org/wiki/Law_of_large_numbers">Law of Large Numbers</a></em> and the <em><a class="reference external" href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit Theorem</a></em> are two important theorems in statistics. The Law of Large Numbers states states that as the number of samples <span class="math notranslate nohighlight">\(n\)</span> of a random variable <span class="math notranslate nohighlight">\(X\)</span> increases, the average of these samples approaches the distribution mean <span class="math notranslate nohighlight">\(\mu = \mathbb{E}[X]\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{For samples } x_1, x_2, ..., x_n,\quad \sum_{i=1}^n \frac{x_i}{n} \rightarrow \mu \quad\text{ as }\quad n \rightarrow \infty.\]</div>
<p>The Central Limit Theorem generalizes the Law of large numbers. It states that for a set of <span class="math notranslate nohighlight">\(n\)</span> independent samples <span class="math notranslate nohighlight">\(x_1, x_2, ..., x_n\)</span> from <em>any</em> random variable <span class="math notranslate nohighlight">\(X\)</span> with bounded mean <span class="math notranslate nohighlight">\(\mu_X\)</span> and variance <span class="math notranslate nohighlight">\(\sigma_X\)</span>, the sample mean random variable <span class="math notranslate nohighlight">\(\bar{X}_n \sim \sum_{i=1}^n x_i/n\)</span> is such that:</p>
<div class="math notranslate nohighlight">
\[\sqrt{n}(\bar{X}_n - \mu_X)\ \underset{distribution}{\longrightarrow}\ \text{Normal}(\mu=0, \sigma=\sigma_X)\]</div>
<p>This theorem is useful for quantifying the uncertainty of the sample mean. If we divide both sides by <span class="math notranslate nohighlight">\(\sqrt{n}\)</span> and shift by <span class="math notranslate nohighlight">\(\mu_X\)</span>, we see that:</p>
<div class="math notranslate nohighlight">
\[\bar{X}_n \sim \text{Normal}(\mu=\mu_X,\sigma=\sigma_X/\sqrt{N})\]</div>
<p>In other words, the standard deviation of the sample mean <span class="math notranslate nohighlight">\(\bar{x} = \sum_{i=1}^n x_i/n\)</span> is roughly <span class="math notranslate nohighlight">\(\sigma_X/\sqrt{n}\)</span>. This relation quantifies the uncertainty of using the sample mean as an estimate of a population mean.</p>
</section>
<section id="hypothesis-testing">
<h2>Hypothesis Testing<a class="headerlink" href="#hypothesis-testing" title="Link to this heading">#</a></h2>
<p>An important part of doing science is the testing of hypotheses. The standard way of doing this is through the steps of the <em>scientific method</em>: Formulate a research question, propose a hypothesis, design an experiment, collect experimental data, analyze the results, and report conclusions. In the analysis of our data, how do we know if our hypothesis is correct? There are many different statistical methods we can apply to test a given hypothesis, each with different strengths and weaknesses. In machine learning, we often use hypothesis testing to determine (hopefully with a high degree of certainty) whether one model is more accurate than another. We can also use hypothesis testing to determine which data features are more significant than other data features when making predictions.</p>
<p>Typically, hypothesis testing involves two competing hypotheses: the <em>null hypothesis</em> (denoted <span class="math notranslate nohighlight">\(H_0\)</span>) and the <em>alternative hypothesis</em> (denoted <span class="math notranslate nohighlight">\(H_1\)</span>). The null hypothesis often is a statement of the “status quo” or a statement of “statistical insignificance”. The alternative hypothesis is the statement of “statistical significance” we are often trying to prove is true. To better illustrate the process of hypothesis testing, we will use the following example:</p>
<section id="example-conductor-vs-insulator-classifier">
<h3>Example: Conductor vs. Insulator Classifier<a class="headerlink" href="#example-conductor-vs-insulator-classifier" title="Link to this heading">#</a></h3>
<p>Suppose we are developing a classifier model that predicts whether a material is a conductor or an insulator. For simplicity, we shall assume that roughly half of all materials are insulators and half are insulators. Our two competing hypotheses would then be:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0\)</span>: The accuracy of our classifier is the same as random guessing (accuracy = 0.5)</p></li>
<li><p><span class="math notranslate nohighlight">\(H_1\)</span>: The accuracy of our classifier is better than than random guessing (accuracy &gt; 0.5)</p></li>
</ul>
<p>Suppose that in order to test our alternative hypothesis <span class="math notranslate nohighlight">\(H_1\)</span>, we compile a dataset of 40 materials (20 conductors and 20 insulators) and use these to evaluate our model. We find that the model has an accuracy of 0.6, meaning it correctly classifies <span class="math notranslate nohighlight">\(60\%\)</span> of the dataset. Since the accuracy is greater than 0.5, does this mean we immediately reject <span class="math notranslate nohighlight">\(H_0\)</span> in favor of <span class="math notranslate nohighlight">\(H_1\)</span>? Not necessarily; it could be the case that our model simply got lucky and “randomly guessed” the classification of more than <span class="math notranslate nohighlight">\(50\%\)</span> of the dataset.</p>
<p>First, let’s consider the distribution of accuracies that could be attained by a random guessing strategy. If we treat each guess as one of <span class="math notranslate nohighlight">\(N = 40\)</span> trials with a probability <span class="math notranslate nohighlight">\(p = 0.5\)</span> of succeeding, we can model the distribution of random guessing strategies with a binomial distribution. Let’s write some Python code to visualize this distribution:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">binom</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># experiment parameters:</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">p_guess_correct</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">model_accuracy</span> <span class="o">=</span> <span class="mf">0.6</span>

<span class="c1"># evaluate distribution:</span>
<span class="n">n_correct</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">n_correct</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="n">N</span><span class="p">,</span><span class="n">p</span><span class="o">=</span><span class="n">p_guess_correct</span><span class="p">)</span>
<span class="n">accuracies</span> <span class="o">=</span> <span class="n">n_correct</span> <span class="o">/</span> <span class="n">N</span>

<span class="c1"># plot distribution</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">accuracies</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">N</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">model_accuracy</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;P(Accuracy)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/bd0d174bf5fabfce06801ccfcc84ba5a23ec4d26cff3fda4a8263d3c7ccb6e19.png" src="../_images/bd0d174bf5fabfce06801ccfcc84ba5a23ec4d26cff3fda4a8263d3c7ccb6e19.png" />
</div>
</div>
<p>In order to evaluate whether or not our result is statistically significant, we will compute the <a class="reference external" href="https://en.wikipedia.org/wiki/P-value"><em><span class="math notranslate nohighlight">\(p\)</span>-value</em></a> associated with our hypothesis testing. A <span class="math notranslate nohighlight">\(p\)</span>-value is a quantity between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span> that describes the probability of obtaining a result at least as extreme as the experimentally observed value assuming that <span class="math notranslate nohighlight">\(H_0\)</span> is true. Roughly speaking, we can interpret a <span class="math notranslate nohighlight">\(p\)</span>-value as the probability of observing the experimental data “by coincidence” if <span class="math notranslate nohighlight">\(H_0\)</span> is in fact true. If a <span class="math notranslate nohighlight">\(p\)</span>-value is low, it means that the alternative hypothesis <span class="math notranslate nohighlight">\(H_1\)</span> is likely to be true. In most research settings, a p-value of at most <span class="math notranslate nohighlight">\(0.05\)</span> (<span class="math notranslate nohighlight">\(5\%\)</span> chance of coincidence) is considered sufficient to show that the alternative hypothesis <span class="math notranslate nohighlight">\(H_1\)</span> is true.</p>
<p>From inspecting this plot we see that the accuracy distribution is approximately normal, having mean <span class="math notranslate nohighlight">\(\mu_X \approx p = 0.5\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2_X \approx p(1-p) = 0.25\)</span>. Per the Central Limit Theorem, we conclude that the estimated accuracy of random guessing is normally distributed with mean <span class="math notranslate nohighlight">\(\mu = \mu_X\)</span> and <span class="math notranslate nohighlight">\(\sigma = \sigma_X/\sqrt{40}\)</span>. The <span class="math notranslate nohighlight">\(p\)</span>-value corresponds to the area under this normal distribution curve corresponding to accuracies with <span class="math notranslate nohighlight">\(0.6\)</span> or greater. Using the values from the previous code cell, we can compute the <span class="math notranslate nohighlight">\(p\)</span>-value as follows:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">norm</span>

<span class="c1"># plot approximate normal distribution via CLT:</span>
<span class="n">mu_clt</span> <span class="o">=</span> <span class="n">p_guess_correct</span>
<span class="n">sigma_clt</span> <span class="o">=</span> <span class="n">p_guess_correct</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p_guess_correct</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

<span class="c1"># evaluate CLT normal distribution:</span>
<span class="n">x_pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">mu_clt</span><span class="o">-</span><span class="mi">4</span><span class="o">*</span><span class="n">sigma_clt</span><span class="p">,</span> <span class="n">mu_clt</span><span class="o">+</span><span class="mi">4</span><span class="o">*</span><span class="n">sigma_clt</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">x_probs</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_pts</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu_clt</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma_clt</span><span class="p">)</span>

<span class="c1"># determine region to the right of estimated accuracy:</span>
<span class="n">pval_pts</span> <span class="o">=</span> <span class="n">x_pts</span><span class="p">[</span><span class="n">x_pts</span> <span class="o">&gt;</span> <span class="n">model_accuracy</span><span class="p">]</span>
<span class="n">pval_probs</span> <span class="o">=</span> <span class="n">x_probs</span><span class="p">[</span><span class="n">x_pts</span> <span class="o">&gt;</span> <span class="n">model_accuracy</span><span class="p">]</span>

<span class="c1"># estimate pvalue:</span>
<span class="n">pvalue</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">model_accuracy</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu_clt</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma_clt</span><span class="p">)</span>

<span class="c1"># plot CLT normal distribution and shade region</span>
<span class="c1"># to the right of estimated accuracy:</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pts</span><span class="p">,</span> <span class="n">x_probs</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">pval_pts</span><span class="p">,</span> <span class="n">pval_probs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;p-value: </span><span class="si">{</span><span class="n">pvalue</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">model_accuracy</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/8f1530b2372477a69e33dc28c14bb4d1ee1aa9b5b4671ee3d92a53f86f04dd23.png" src="../_images/8f1530b2372477a69e33dc28c14bb4d1ee1aa9b5b4671ee3d92a53f86f04dd23.png" />
</div>
</div>
<p>Since the <span class="math notranslate nohighlight">\(p\)</span>-value is <span class="math notranslate nohighlight">\(0.006 \le 0.05\)</span>, we conclude that the <span class="math notranslate nohighlight">\(H_1\)</span> is true, meaning the accuracy of our model (<span class="math notranslate nohighlight">\(0.6\)</span>) being greater than random guessing (<span class="math notranslate nohighlight">\(0.5\)</span>) is statistically significant. This proves that the model is better than random guessing; however it is worth noting that a model with an accuracy of <span class="math notranslate nohighlight">\(0.6\)</span> may not be practically useful for distinguishing between insulators and metals.</p>
</section>
</section>
<section id="the-multivariate-normal-distribution">
<h2>The Multivariate Normal Distribution:<a class="headerlink" href="#the-multivariate-normal-distribution" title="Link to this heading">#</a></h2>
<p>Often, we will find that we are working with multi-dimensional data where correlations may exist between more than one variable. Fortunately, these correlations can be described by a <a class="reference external" href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">multivariate normal</a> distribution. Like the 1-dimensional normal distribution, the multivariate normal distribution is characterized by two parameters, a mean vector <span class="math notranslate nohighlight">\({\boldsymbol{\mu}}\)</span> and a <a class="reference external" href="https://en.wikipedia.org/wiki/Covariance_matrix">covariance matrix</a> <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span>. For a <span class="math notranslate nohighlight">\(d\)</span>-dimensional distribution, these parameters can be written in matrix form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{\mu} = \begin{bmatrix} \mu_1 \\ \mu_2 \\ \vdots \\ \mu_d \end{bmatrix}, \qquad\qquad \mathbf{\Sigma} = \begin{bmatrix}
\sigma_{1}^2 &amp; \sigma_{12} &amp; \dots &amp; \sigma_{1d} \\
\sigma_{21} &amp; \sigma_{2}^2 &amp; \dots &amp; \sigma_{2d} \\
\vdots      &amp; \vdots      &amp; \ddots &amp; \vdots \\
\sigma_{d1} &amp; \sigma_{d2} &amp; \dots &amp; \sigma_{d}^2
\end{bmatrix}\end{split}\]</div>
<p>(For a review of matrices and matrix-vector products see the next section.) The entries <span class="math notranslate nohighlight">\(\mu_i = \mathbb{E}[X_i]\)</span> are the coordinates of the mean <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span>. The entries <span class="math notranslate nohighlight">\(\sigma_i^2 = \mathbb{E}[(X_i - \mu_i)^2]\)</span> in <span class="math notranslate nohighlight">\(\Sigma\)</span> are the variances of each individual component of the distribution. Finally, the off-diagonal components <span class="math notranslate nohighlight">\(\sigma_{ij}\)</span> are the <em>covariances</em> of components <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>. The covariance of two components is given by:</p>
<div class="math notranslate nohighlight">
\[\text{Cov}(X_i,X_j) = \mathbb{E}[(X_i - \mu_i)(X_j - \mu_j)] = \iint_{\mathcal{X}_i \times \mathcal{X_j}} p(x_i,x_j)(x_i - \mu_i)(x_j - \mu_j)\ dx_jdx_i\]</div>
<p>The probability distribution of a multivariate normal distribution is given by:</p>
<div class="math notranslate nohighlight">
\[p(\mathbf{x}) = \frac{1}{\sqrt{(2\pi)^d \det(\Sigma)}} \exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu})\right)\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>From the definition of <span class="math notranslate nohighlight">\(\text{Cov}(X_i, X_j)\)</span>, it follows that <span class="math notranslate nohighlight">\(\text{Cov}(X_i, X_j) = \text{Cov}(X_j, X_i)\)</span>. This means that the covariance matrix <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span> is symmetric (<span class="math notranslate nohighlight">\(\mathbf{\Sigma} = \mathbf{\Sigma}^T\)</span>), having up to <span class="math notranslate nohighlight">\(d(d+1)/2\)</span> distinct values that need to be determined.</p>
<p>For any two random variables, if <span class="math notranslate nohighlight">\(\text{Cov}(A,B) = 0\)</span> the random variables are uncorrelated; otherwise, the sign of <span class="math notranslate nohighlight">\(\text{Cov}(A,B)\)</span> indicates whether <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are positively or negatively correlated.</p>
<p>Also, for the multivariate normal distribution to be well-defined, we must impose that the matrix <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span> is invertible. If <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span> is not invertible, <span class="math notranslate nohighlight">\(\det(\mathbf{\Sigma}) = 0\)</span>, which means <span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span> cannot be normalized.</p>
</div>
<p>To evaluate the density of a multivariate normal distribution, we can use the <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html"><code class="docutils literal notranslate"><span class="pre">scipy.stats.multivariate_normal.pdf</span></code></a> function:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">multivariate_normal</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># mean of distribution:</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span> <span class="p">])</span>

<span class="c1"># covariance matrix of distribution:</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span>  <span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span> <span class="p">],</span>
    <span class="p">[</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span>  <span class="mf">2.0</span> <span class="p">]</span>
<span class="p">])</span>

<span class="c1"># define 2D mesh grid:</span>
<span class="n">x1_pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">x2_pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">x1_mesh</span><span class="p">,</span> <span class="n">x2_mesh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1_pts</span><span class="p">,</span> <span class="n">x2_pts</span><span class="p">)</span>
<span class="n">x12_mesh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">((</span><span class="n">x1_mesh</span><span class="p">,</span> <span class="n">x2_mesh</span><span class="p">))</span>

<span class="c1"># evaluate probability density on mesh points:</span>
<span class="n">prob_mesh</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x12_mesh</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>

<span class="c1"># plot distribution:</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x1_mesh</span><span class="p">,</span> <span class="n">x2_mesh</span><span class="p">,</span> <span class="n">prob_mesh</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;p(x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/8c85af9fbc6bb888b06e30ccfc24bd3942294f90ac3d5c2ab369564b4c20314d.png" src="../_images/8c85af9fbc6bb888b06e30ccfc24bd3942294f90ac3d5c2ab369564b4c20314d.png" />
</div>
</div>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Exercise 1: Comparing Two Classifiers:</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Consider the Conductor vs. Insulator Classifier example from above. Suppose that after some additional work, we propose an improved classifier model, which we think can classify materials with an accuracy greater than <span class="math notranslate nohighlight">\(0.6\)</span>. We now have the following two Hypotheses:</p>
<ul class="simple">
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(H_0\)</span>: The accuracy of the improved classifier is the same as the original classifier (accuracy = 0.6)</p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(H_1\)</span>: The accuracy of the improved classifier is the greater than the original classifier (accuracy &gt; 0.6)</p></li>
</ul>
<p class="sd-card-text">Suppose we evaluate the accuracy of the improved classifier with a dataset of only 40 materials, and find that <span class="math notranslate nohighlight">\(26\)</span> of the <span class="math notranslate nohighlight">\(40\)</span> materials are classified correctly. Repeat the same analysis as before to determine whether <span class="math notranslate nohighlight">\(H_1\)</span> is true and report the <span class="math notranslate nohighlight">\(p\)</span>-value.</p>
<p class="sd-card-text">Next, suppose that we instead used a dataset of <span class="math notranslate nohighlight">\(80\)</span> materials and found that <span class="math notranslate nohighlight">\(52\)</span> of these were classified correctly (same accuracy as before, but more data). Does this change our conclusion as to whether or not <span class="math notranslate nohighlight">\(H_1\)</span> is true?</p>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Exercise 2: Fitting a Multivariate Normal Distribution:</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Generate a 2D dataset of 10,000 random points uniformly sampled within a rectangle of width <span class="math notranslate nohighlight">\(2\)</span> and height <span class="math notranslate nohighlight">\(1\)</span>, where the lower left hand corner of the rectangle is fixed at the origin. You can generate uniform values on the interval <span class="math notranslate nohighlight">\([a,b]\)</span> using the <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html"><code class="docutils literal notranslate"><span class="pre">np.random.uniform</span></code></a> function in the numpy package.</p>
<p class="sd-card-text">Fit this distribution to a multivariate normal distribution by computing the sample mean vector <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> and sample covariance matrix <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span>. You can do this with the <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.mean.html"><code class="docutils literal notranslate"><span class="pre">np.mean</span></code></a> function and the <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.cov.html"><code class="docutils literal notranslate"><span class="pre">np.cov</span></code></a> functions respectively (these are also contained in the <code class="docutils literal notranslate"><span class="pre">numpy</span></code> package).</p>
<p class="sd-card-text">Plot both the generated data points and the fitted multivariate normal distribution using <a class="reference external" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contourf.html"><code class="docutils literal notranslate"><span class="pre">plt.contourf</span></code></a> or a similar function.</p>
</div>
</details><section id="solutions">
<h3>Solutions:<a class="headerlink" href="#solutions" title="Link to this heading">#</a></h3>
<section id="exercise-1-comparing-two-classifiers">
<h4>Exercise 1: Comparing Two Classifiers<a class="headerlink" href="#exercise-1-comparing-two-classifiers" title="Link to this heading">#</a></h4>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">norm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Original model accuracy:</span>
<span class="n">original_accuracy</span> <span class="o">=</span> <span class="mf">0.6</span>

<span class="c1"># Experimental results:</span>
<span class="c1"># (number correct, total)</span>
<span class="n">experiment_results</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="mi">26</span><span class="p">,</span><span class="mi">40</span><span class="p">),</span> <span class="p">(</span><span class="mi">52</span><span class="p">,</span><span class="mi">80</span><span class="p">)</span>
<span class="p">]</span>

<span class="c1"># perform analysis for N = 40 and 80:</span>
<span class="k">for</span> <span class="p">(</span><span class="n">n_correct</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span> <span class="ow">in</span> <span class="n">experiment_results</span><span class="p">:</span>

    <span class="c1"># compute accuracy:</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">n_correct</span><span class="o">/</span><span class="n">N</span>

    <span class="c1"># compute CLT mu and sigma:</span>
    <span class="n">mu_clt</span> <span class="o">=</span> <span class="n">original_accuracy</span>
    <span class="n">sigma_clt</span> <span class="o">=</span> <span class="n">original_accuracy</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">original_accuracy</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

    <span class="c1"># evaluate CLT normal distribution:</span>
    <span class="n">x_pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">mu_clt</span><span class="o">-</span><span class="mi">4</span><span class="o">*</span><span class="n">sigma_clt</span><span class="p">,</span> <span class="n">mu_clt</span><span class="o">+</span><span class="mi">4</span><span class="o">*</span><span class="n">sigma_clt</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="n">x_probs</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_pts</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu_clt</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma_clt</span><span class="p">)</span>

    <span class="c1"># determine region to the right of estimated accuracy:</span>
    <span class="n">pval_pts</span> <span class="o">=</span> <span class="n">x_pts</span><span class="p">[</span><span class="n">x_pts</span> <span class="o">&gt;</span> <span class="n">accuracy</span><span class="p">]</span>
    <span class="n">pval_probs</span> <span class="o">=</span> <span class="n">x_probs</span><span class="p">[</span><span class="n">x_pts</span> <span class="o">&gt;</span> <span class="n">accuracy</span><span class="p">]</span>

    <span class="c1"># estimate pvalue:</span>
    <span class="n">pvalue</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu_clt</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma_clt</span><span class="p">)</span>

    <span class="c1"># plot CLT normal distribution and shade region</span>
    <span class="c1"># to the right of estimated accuracy:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;N = </span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1"> Dataset&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pts</span><span class="p">,</span> <span class="n">x_probs</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">pval_pts</span><span class="p">,</span> <span class="n">pval_probs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;p-value: </span><span class="si">{</span><span class="n">pvalue</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model Accuracy&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="c1"># print out conclusion:</span>
    <span class="k">if</span> <span class="n">pvalue</span> <span class="o">&gt;</span> <span class="mf">0.05</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;pvalue &gt; 0.05, so we reject H0&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;pvalue &lt; 0.05, so we accept H0&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4f047181ec5ce3a2fca703601c5e00edc5678798e1a2f96ecdf7dfe3c946aaef.png" src="../_images/4f047181ec5ce3a2fca703601c5e00edc5678798e1a2f96ecdf7dfe3c946aaef.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>pvalue &gt; 0.05, so we reject H0
</pre></div>
</div>
<img alt="../_images/ea356588ee41974e3ed0a99820bcf65a55fdc65f1178c2ededf58b185be83553.png" src="../_images/ea356588ee41974e3ed0a99820bcf65a55fdc65f1178c2ededf58b185be83553.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>pvalue &lt; 0.05, so we accept H0
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="exercise-2-fitting-a-multivariate-normal-distribution">
<h4>Exercise 2: Fitting a Multivariate Normal Distribution:<a class="headerlink" href="#exercise-2-fitting-a-multivariate-normal-distribution" title="Link to this heading">#</a></h4>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">multivariate_normal</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># randomly sample the x1 and x2 coordinates of points:</span>
<span class="n">n_data</span>  <span class="o">=</span> <span class="mi">1000</span>
<span class="n">x1_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">n_data</span><span class="p">)</span>
<span class="n">x2_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n_data</span><span class="p">)</span>

<span class="c1"># combine x1 and x2 components:</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="n">x1_data</span><span class="p">,</span> <span class="n">x2_data</span> <span class="p">])</span>
<span class="n">x_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x_cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span>

<span class="c1"># define 2D mesh grid:</span>
<span class="n">x1_pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">x2_pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">x1_mesh</span><span class="p">,</span> <span class="n">x2_mesh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1_pts</span><span class="p">,</span> <span class="n">x2_pts</span><span class="p">)</span>
<span class="n">x12_mesh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">([</span><span class="n">x1_mesh</span><span class="p">,</span> <span class="n">x2_mesh</span><span class="p">])</span>

<span class="c1"># evaluate probability density on mesh points:</span>
<span class="n">prob_mesh</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x12_mesh</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">x_mean</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">x_cov</span><span class="p">)</span>

<span class="c1"># show plot of fitted multivariate normal distribution:</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x1_mesh</span><span class="p">,</span> <span class="n">x2_mesh</span><span class="p">,</span> <span class="n">prob_mesh</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x1_data</span><span class="p">,</span> <span class="n">x2_data</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/770571bb61f55a0bcb945dce99d2066fe916ca6f5846bafaba702480454e6604.png" src="../_images/770571bb61f55a0bcb945dce99d2066fe916ca6f5846bafaba702480454e6604.png" />
</div>
</details>
</div>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./ml_intro"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ml_intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Introduction to Machine Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="math_review.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Mathematics Review</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-distributions">Probability Distributions:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-binomial-distribution">The Binomial Distribution:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-normal-distribution">The Normal Distribution:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-central-limit-theorem">The Central Limit Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hypothesis-testing">Hypothesis Testing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-conductor-vs-insulator-classifier">Example: Conductor vs. Insulator Classifier</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-multivariate-normal-distribution">The Multivariate Normal Distribution:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions">Solutions:</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-comparing-two-classifiers">Exercise 1: Comparing Two Classifiers</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-fitting-a-multivariate-normal-distribution">Exercise 2: Fitting a Multivariate Normal Distribution:</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Colin Burdine, E. P. Blair, and Nishat-Tasnim Liza
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>